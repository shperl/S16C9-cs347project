<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (version 1.7.0_80) on Thu Dec 03 15:37:33 EST 2015 -->
<title>hadoop.hive.table (Oracle NoSQL Database Examples)</title>
<meta name="date" content="2015-12-03">
<link rel="stylesheet" type="text/css" href="../../../style.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="hadoop.hive.table (Oracle NoSQL Database Examples)";
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../overview-summary.html">Overview</a></li>
<li class="navBarCell1Rev">Package</li>
<li>Class</li>
<li><a href="package-use.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html">Help</a></li>
</ul>
<div class="aboutLanguage"><em><b>Oracle NoSQL Database Examples</b><br><font size=\"-1\"> version 12cR1.3.5.2</font>
      </em></div>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../hadoop/package-summary.html">Prev Package</a></li>
<li><a href="../../../hadoop/table/package-summary.html">Next Package</a></li>
</ul>
<ul class="navList">
<li><a href="../../../index.html?hadoop/hive/table/package-summary.html" target="_top">Frames</a></li>
<li><a href="package-summary.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<div class="header">
<h1 title="Package" class="title">Package&nbsp;hadoop.hive.table</h1>
<div class="docSummary">
<div class="block"><em>The Table API Hive Cookbook</em>: documentation that describes how to run example 
<a href="https://hive.apache.org"><b><code>Apache Hive</code></b></a> or 
<a href="http://www.oracle.com/us/products/database/big-data-sql/overview/index.html"><b><code>Oracle Big Data SQL</code></b></a> 
queries against data written via the Oracle NoSQL Database Table API.</div>
</div>
<p>See:&nbsp;<a href="#package_description">Description</a></p>
</div>
<div class="contentContainer">
<ul class="blockList">
<li class="blockList">
<table class="packageSummary" border="0" cellpadding="3" cellspacing="0" summary="Class Summary table, listing classes, and an explanation">
<caption><span>Class Summary</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Class</th>
<th class="colLast" scope="col">Description</th>
</tr>
<tbody>
<tr class="altColor">
<td class="colFirst"><a href="../../../hadoop/hive/table/LoadRmvTable.html" title="class in hadoop.hive.table">LoadRmvTable</a></td>
<td class="colLast">
<div class="block">Class that creates an example table in a given NoSQL Database store and
 then uses the Table API to populate the table with sample records.</div>
</td>
</tr>
</tbody>
</table>
</li>
</ul>
<a name="package_description">
<!--   -->
</a>
<h2 title="Package hadoop.hive.table Description">Package hadoop.hive.table Description</h2>
<div class="block"><em>The Table API Hive Cookbook</em>: documentation that describes how to run example 
<a href="https://hive.apache.org"><b><code>Apache Hive</code></b></a> or 
<a href="http://www.oracle.com/us/products/database/big-data-sql/overview/index.html"><b><code>Oracle Big Data SQL</code></b></a> 
queries against data written via the Oracle NoSQL Database Table API.

<a name="introduction_hive"/>
<h3>Introduction</h3>

With the introduction of the 
<a href="../../../../javadoc/oracle/kv/hadoop/table/package-summary.html"><b><code>Oracle NoSQL Database Hadoop integration classes</code></b></a>, 
which support running Hadoop MapReduce jobs against data stored in an 
Oracle NoSQL Database table, it was natural to also provide new interfaces 
and classes which support running Hive queries against such table data
(since a typical Hive query generally results in the execution of a 
MapReduce job). In addition to describing the core interfaces and classes 
involved in running a Hive query against data from a table located in a
given Oracle NoSQL Database store (a <em>KVStore</em>), the information
presented below also walks through the steps to take to execute a 
given set of basic Hive queries against example table data contained in
such a store; either <b><em>secure</em></b> 
or <em><b>non</b>-secure</em>.

<a name="prerequisites_hive"/>
<h3>Prerequisites</h3>

Before attempting to execute the example that demonstrates the concepts
presented in this document, you should first satisfy the following 
prerequisites:

<ul>
  <li>Become familiar with both <a href="https://hive.apache.org"><b><code>Apache Hive</code></b></a>
       and its programming model; that is, become familiar with how to write
       and execute a Hive query.
  <li>Become familiar with <a href="http://en.wikipedia.org/wiki/Apache_Hadoop"><b><code>Apache Hadoop</code></b></a>;
       specifically, become familiar with how Hive and Hadoop interact.
  <li>Deploy a Hadoop cluster with 3 <em>DataNodes</em> running on 
       machines with host names, <em>dn-host-1</em>, <em>dn-host-2</em>, 
       and <em>dn-host-3</em>.
  <li>Install and configure Hive, with network access to the Hadoop cluster 
       that was deployed; specifying either Derby (the default) or MySQL to 
       store the Hive metadata.
  <li>Become familiar with the Hive Command Line Interface (the <em>Hive CLI</em>),
       and the Hive Query Language.
  <li>Become familiar with Oracle NoSQL Database and then install, start,
       and configure an Oracle NoSQL Database 
       &mdash; with <b>&lt;KVHOME&gt;</b> equal to <code>/opt/ondb/kv</code> &mdash;
       that is network reachable from the nodes of the Hadoop cluster.
  <li>Deploy a KVStore 
       &mdash; named <em>example-store</em>, with <b>&lt;KVROOT&gt;</b> equal to <code>/opt/ondb/example-store</code> &mdash; 
       to 3 machines (real or virtual) with host names, <em>kv-host-1</em>,
       <em>kv-host-2</em>, and <em>kv-host-3</em>;  where an admin service,
       listening on port <em>5000</em>, is deployed on each host.
  <li>Become familiar with the Oracle NoSQL Database Security model and
       be able to configure the deployed store for secure access (optional).
  <li>If the deployed store is configured for secure access, become familiar 
       with the Oracle NoSQL Database Administrative CLI, start it, and 
       then follow the steps presented in 
       <a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>Appendix B</code></b></a> 
       of the <a href="../../table/package-summary.html"><b><code>Hadoop/Table API Example</code></b></a> to securely connect to the 
       store's admin service and create a KVStore user named <em>example-user</em>; 
       along with the appropriate security artifacts (<em>login file</em>, 
       <em>trust file</em>, and either <em>password file</em> or 
       <em>Oracle Wallet</em> [Enterprise Edition only]).
  <li>Become familiar with the supporting Java classes
       from the <em>Hadoop/Table API Example</em>, and then follow the 
       <a href="../../table/package-summary.html#create_populate_vehicle_table"><b><code>steps</code></b></a> 
       presented in that example to create and populate a table named 
       <em>vehicleTable</em> with example data consisting of only
       primitive data types.
  <li>Become familiar with the <code>LoadRmvTable</code> program provided
       with this example, and then follow the steps presented in the sections of 
       <a href="#appendix_a_create_populate_rmv_table"><b><code>Appendix A</code></b></a> 
       of this document to create and populate a table named <em>rmvTable</em> with example 
       data consisting of both primitive and non-primitive data types.
</ul>

Using specific values for items such as the <code>&lt;KVHOME&gt;</code>
and <code>&lt;KVROOT&gt;</code> environment variables, the store name, 
host names, and admin port described above should allow you to more easily 
follow the example that is presented. Combined with the information contained 
in the <em>Oracle NoSQL Database Getting Started Guide</em>, as well as the 
<em>Oracle NoSQL Database Admin Guide</em>, the <em>Oracle NoSQL Database Security Guide</em> 
and the
<a href="../../table/package-summary.html"><b>Hadoop/Table API Example</b></a>,
you should then be able to generalize and extend the examples presented here to your own 
particular development scenario; substituting the values specific to the 
given environment where necessary. 
<p>
Note that detailed instructions for deploying a non-secure KVStore are provided in 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>Appendix A</code></b></a> 
of the <em>Hadoop/Table API Example</em>. Similarly, 
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>Appendix B</code></b></a> 
of the <em>Hadoop/Table API Example</em> provides instructions for deploying a 
KVStore configured for security.

<a name="primer_hive"/>
<h3>A Brief Hive Primer</h3>

Paraphrasing wikipedia,
<a href="https://hive.apache.org"><b><code>Apache Hive</code></b></a> 
is a data warehouse infrastructure built on top of 
<a href="http://en.wikipedia.org/wiki/Apache_Hadoop"><b><code>Apache Hadoop</code></b></a> 
that facilitates querying datasets residing in distributed file systems or data stores such as 
<a href="http://en.wikipedia.org/wiki/Apache_Hadoop#HDFS"><b><code>Hadoop HDFS</code></b></a> 
or 
<a href="http://en.wikipedia.org/wiki/Amazon_S3"><b><code>Amazon S3</code></b></a>.
Additionally, Hive also provides a pluggable programming model that allows
you to specify custom interfaces and classes that support querying
data residing in data sources other than HDFS and S3; in particular, 
<b><em>data written to an Oracle NoSQL Database table</em></b>.
To access and analyze data stored in these data sources, Hive provides a 
mechanism to project structure onto the data and query the data using 
a SQL-like language called the <em>Hive Query Language</em>, or 
<em>HQL</em>. Depending on the complexity of a given Hive query,
the Hive infrastructure may construct and deploy a set of MapReduce 
jobs to retrieve and process the data or, when possible, it may simply 
satisfy the query via the metadata stored in the Hive <em>metastore</em> 
(Derby or MySQL).
<p>
In addition to the Hive infrastructure itself, Hive also provides a 
convenient client-side command line interface (the <em>Hive CLI</em>);
which allows you to interact with the Hive infrastructure to create  
a Hive <em>external table</em> and then map it to the data located 
in a source like those described above.
<p>
As indicated above, a set of interfaces and classes that satisfy the 
Hive programming model are provided by Oracle NoSQL Database which support 
running Hive queries against table data contained in a KVStore 
&mdash; either <em><b>secure</em></b> or <em><b>non</b>-secure</em>. 
These classes are located in the 
<a href="../../../../javadoc/oracle/kv/hadoop/hive/table/package-summary.html"><b><code>oracle.kv.hadoop.hive.table</code></b></a>
package, and consist of the following Hive and Hadoop types:

<ul>
  <li>A subclass of the Hive class, 
       <a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.html"><b><code>org.apache.hadoop.hive.ql.metadata.HiveStorageHandler</code></b></a>,
       which is the mechanism (pluggable interface) to use to specify the 
       location of the data the Hive infrastructure should process, as well 
       as how to process that data; and which consists of the following components: 

<a name="hadoop_mrv1_classes"/>
    <ul>
      <li>A subclass of the Hadoop MapReduce 
        <a href="#yarn_versus_mrv1"><b><em>version 1</em></b></a> class,
        <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html"><b><code>org.apache.hadoop.mapred.InputFormat</code></b></a>,
       which specifies how the associated MapReduce job reads its input data
       (from an Oracle NoSQL Database table).
      <li>A subclass of the Hadoop MapReduce 
        <a href="#yarn_versus_mrv1"><b><em>version 1</em></b></a> class,
        <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/OutputFormat.html"><b><code>org.apache.hadoop.mapred.OutputFormat</code></b></a>,
       which specifies how the associated MapReduce job writes its output.
      <li>A subclass of the Hive class, 
       <a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/serde2/SerDe.html"><b><code>org.apache.hadoop.hive.serde2.SerDe</code></b></a>,
       which is used to deserialize the table data that is retrieved and
       sent to the Hive infrastructure and/or Hadoop MapReduce job for processing, 
       and to serialize data input to Hive that will be written to the table (currently not supported).
      <li>Metadata <em>hooks</em> for keeping an external catalog in sync with the Hive metastore.
      <li>Rules for setting up the configuration properties on MapReduce jobs run against the data being processed.
    </ul>
  <li>A subclass of the Hadoop 
       <a href="#yarn_versus_mrv1"><b><em>version 1</em></b></a> class,
       <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/RecordReader.html"><b><code>org.apache.hadoop.mapred.RecordReader</code></b></a>,
       which specifies how the mapped keys and values are located and retrieved 
       during any MapReduce processing performed as part of executing a Hive query.
  <li>A subclass of the Hadoop 
       <a href="#yarn_versus_mrv1"><b><em>version 1</em></b></a> class,
       <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputSplit.html"><b><code>org.apache.hadoop.mapred.InputSplit</code></b></a>,
       which represents the data to be processed by an individual <b><em>Mapper</em></b> 
       that operates during the MapReduce processing performed as part of 
       executing a Hive query.
</ul>

As described below, it is through the specific implementation of the 
<a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.html"><b><code>HiveStorageHandler</code></b></a>
provided in the Oracle NoSQL Database distribution that the Hive infrastructure 
obtains access to a given KVStore and the table data on which to run 
the desired Hive query.

<a name="ondb_hive_integration_classes"/>
<h3>The Oracle NoSQL Database Table API Hive Integration Classes</h3>

To support running Hive queries against data stored in a table of an 
Oracle NoSQL Database KVStore, the following core classes are employed: 

<ul>
  <li> <a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableStorageHandler.html"><b><code>oracle.kv.hadoop.hive.table.TableStorageHandler</code></b></a>
  <li> <a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableHiveInputFormat.html"><b><code>oracle.kv.hadoop.hive.table.TableHiveInputFormat</code></b></a>
  <li> <a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableHiveInputSplit.html"><b><code>oracle.kv.hadoop.hive.table.TableHiveInputSplit</code></b></a>
  <li> <a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableHiveRecordReader.html"><b><code>oracle.kv.hadoop.hive.table.TableHiveRecordReader</code></b></a>
  <li> <a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableSerDe.html"><b><code>oracle.kv.hadoop.hive.table.TableSerDe</code></b></a>
  <li>Implementations &mdash; specific to Oracle NoSQL Database &mdash; of the Hive 
    <a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspector.html"><b><code>org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector</code></b></a>
    interface that support deserialization of the primitive and non-primitive data types defined by the Oracle NoSQL Database Table API (see below).
</ul>

For more detail about the semantics of the classes listed above, 
refer to the javadoc of each respective class.

<a name="mapping_hive_to_ondb_data_model"/>
<h3>Mapping the Hive Data Model to the Oracle NoSQL Database Table API Data Model</h3>

As the examples below demonstrate, in order to execute a Hive query 
against data stored in an Oracle NoSQL Database table, a Hive <em>external table</em> 
must be created with a schema mapped from the schema of the desired Oracle NoSQL Database 
table. This is accomplished by applying the mapping described here, in which the 
following implementations of the Hive 
<a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspector.html"><b><code>ObjectInspector</code></b></a>
interface are used in the deserialization process to convert the associated data type 
defined by the Oracle NoSQL Database Table API to its corresponding type in the 
Hive data model:

<a name="hive_object_inspector_impls"/>
<ul>
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableBinaryObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableBinaryObjectInspector</code></b></a>.
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableBooleanObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableBooleanObjectInspector</code></b></a>.
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableDoubleObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableDoubleObjectInspector</code></b></a>.
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableFloatObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableFloatObjectInspector</code></b></a>.
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableIntObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableIntObjectInspector</code></b></a>.
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableLongObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableLongObjectInspector</code></b></a>.
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableEnumObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableEnumObjectInspector</code></b></a>
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableArrayObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableArrayObjectInspector</code></b></a>
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableMapObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableMapObjectInspector</code></b></a>
  <li><a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableRecordObjectInspector.html"><b><code>oracle.kv.hadoop.hive.table.TableRecordObjectInspector</code></b></a>
</ul>

Note that Hive's data model specifies many more types than the types defined 
by the Oracle NoSQL Database Table API. As a result, the data model defined 
by the Oracle NoSQL Database Table API (see 
<a href="../../../../javadoc/oracle/kv/table/FieldDef.Type.html"><b><code>oracle.kv.table.FieldDef.Type</code></b></a>) 
is mapped to only a <em>subset</em> of the types defined by Hive's 
data model. Specifically, when creating a Hive external table so that you 
can query the data in a given Oracle NoSQL Database table, the Hive table 
must be created with a schema consistent with the mappings shown in the following 
table:
<p>

<a name="ondb_hive_data_model_mapping_table"/>
<table border="1" style="width:50%;border-collapse:collapse">
  <caption>Oracle NoSQL Database-to-Hive Data Type Mappings</caption>
  <tr>
    <th>Oracle NoSQL Database Table API</th>
    <th>Hive</th>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.STRING</td>
    <td align="center">STRING</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">VARCHAR</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">CHAR</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.BOOLEAN</td>
    <td align="center">BOOLEAN</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.BINARY</td>
    <td align="center">BINARY</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.FIXED_BINARY</td>
    <td align="center">BINARY</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">TINYINT</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">SMALLINT</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.INTEGER</td>
    <td align="center">INT</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.LONG</td>
    <td align="center">BIGINT</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.FLOAT</td>
    <td align="center">FLOAT</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">DECIMAL</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.DOUBLE</td>
    <td align="center">DOUBLE</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.ENUM</td>
    <td align="center">STRING</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">TIMESTAMP</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">DATE</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.ARRAY</td>
    <td align="center">ARRAY</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.MAP</td>
    <td align="center">MAP&lt;STRING, data_type&gt;</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.RECORD</td>
    <td align="center">STRUCT&lt;col_name : data_type, ...&gt;</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">UNIONTYPE&lt;data_type, data_type, ...&gt;</td>
  </tr>
</table>
<p>

It is important to understand that when using Hive to query data in an 
Oracle NoSQL Database table, the schema of the Hive external table you 
create is dependent on the schema of the corresponding Oracle NoSQL Database 
table you wish to query. Thus, if you create a Hive external table with 
a schema that includes a Hive data type that is <em>not</em> mapped 
from an Oracle NoSQL Database  
<a href="../../../../javadoc/oracle/kv/table/FieldDef.Type.html"><b><code>FieldDef.Type</code></b></a>, 
then an error will occur when any attempt is made to query the table.

<p>
<a name="yarn_versus_mrv1"/>
<b><em>&mdash; YARN versus MapReduce version 1 &mdash;</em></b>
<p>

Currently, Hadoop deployments include two versions of MapReduce. 
The first version (referred to as <b><em>MRv1</em></b>) is the 
original version of MapReduce; and consists of interfaces and classes 
from the Java package <code>org.apache.hadoop.<b>mapred</b></code>. 
The newer version of MapReduce is referred to as <b><em>YARN</em></b> 
(<em>Yet Another Resource Negotiator</em>) or, more generally, 
<b><em>MRv2</em></b>; and resides in the package 
<code>org.apache.hadoop.<b>mapreduce</b></code>. Unfortunately, the 
<a href="../../../../javadoc/oracle/kv/hadoop/hive/table/package-summary.html"><b><code>Table API Hive integration</code></b></a>
classes must address the existence of both versions of MapReduce, for the 
following reasons: 

<ul>
  <li>Hive currently employs <em>MRv1</em>, whereas the 
       <a href="../../../../javadoc/oracle/kv/hadoop/table/package-summary.html"><b><code>Oracle NoSQL Database Table API Hadoop integration</code></b></a> classes employ <em>MRv2</em>.
  <li><em>MRv1</em> and <em>MRv2</em> are source incompatible.
</ul>

As a result, the <a href="#ondb_hive_integration_classes"><b><code>core classes</code></b></a> 
listed above must be subclasses of the previously described 
<a href="#hadoop_mrv1_classes"><b><code>MRv1 classes</code></b></a>. 
For example, rather than subclassing the MRv2 based Table API Hadoop integration class 
<a href="../../../../javadoc/oracle/kv/hadoop/table/TableInputFormat.html"><b><code>TableInputFormat</code></b></a> 
(which would be preferred), because of the incompatability between MRv1 and MRv2,
the Table API Hive integration class described above 
(<a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableHiveInputFormat.html"><b><code>TableHiveInputFormat</code></b></a>)
actually subclasses the Hadoop MRv1 class 
<a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html"><b><code>org.apache.hadoop.mapred.InputFormat</code></b></a>. Thus, to exploit and reuse the mechanisms
provided by the 
<a href="../../../../javadoc/oracle/kv/hadoop/table/package-summary.html"><b><code>Table API <em>Hadoop</em> integration</code></b></a>
classes, the 
<a href="../../../../javadoc/oracle/kv/hadoop/hive/table/package-summary.html"><b><code>Table API <em>Hive</em> integration</code></b></a>
classes presented here internally create, manage, and delegate to subclasses of the 
appropriate MRv2 based classes.
<p>
It is also important to note that because the
<a href="../../../../javadoc/oracle/kv/hadoop/table/package-summary.html"><b><code>Table API Hadoop integration</code></b></a>
classes do not currently support writing data from a MapReduce job into
a KVStore, the
<a href="../../../../javadoc/oracle/kv/hadoop/hive/table/package-summary.html"><b><code>Table API Hive integration</code></b></a>
classes do not support queries that <em>modify</em> the contents of a table in a KVStore.

<a name="hive_query_example_1"/>
<h3>Example 1: Executing Hive Queries Against Primitive Data Types</h3>

This first example demonstrates how to execute various Hive queries against a 
simple Oracle NoSQL Database table; defined with a schema consisting of 
<em><b>primitive data types only</b></em>. To run the queries described 
in this example, after satisfying the <a href="#prerequisites_hive"><b><code>prerequisites</code></b></a>
listed above, if not done so already, you should deploy either a 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>non-secure</code></b></a> 
or a  
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>secure</code></b></a> 
KVStore (or both), and then 
<a href="../../table/package-summary.html#create_populate_vehicle_table"><b><code>create and populate</code></b></a> 
a table named <em>vehicleTable</em> in each such store. If any of the stores you
deploy are configured for secure access, then you must also follow the steps provided in 
<a href="#appendix_c_hive_ondb_security"><b><code>Appendix C</code></b></a> 
of this document to configure Hive with the artifacts and environment necessary to interact 
with a secure KVStore. Once these initial steps are performed, you can then create a 
Hive external table mapped to the KVStore <em>vehicleTable</em>, and execute the 
set of example Hive queries (described below) against the data stored in that table.
<p>

<a name="create_hive_external_vehicle_table"/>
<h4>Create a Hive External Table Mapped to the Oracle NoSQL Database 'vehicleTable'</h4> 

After satisfying the above <a href="#prerequisites_hive"><b><code>prerequisites</code></b></a>
related to the Hive programming model and its CLI, you can create a Hive external table 
mapped to the <em>Oracle NoSQL Database</em> 'vehicleTable' located in the desired KVStore, 
and then query the data in that table. To do this, first execute the Hive interactive CLI 
by typing the following at the command line of the Hive client node: 

<pre>
  &gt; hive
</pre>

which should then present the following Hive CLI command prompt:

<pre>
  hive&gt;
</pre>

At this point, the desired Hive commands (table creation and/or queries) can be executed.

<p>
<a name="create_hive_external_vehicle_table_nonsecure"/>
<b><em>&mdash; Creating a Hive External Table Mapped to 'vehicleTable' in a <strong style="color: green;">Non-Secure</strong> KVStore &mdash;</em></b>
<p>

If you will be running your Hive query against a <em>non-secure</em> KVStore 
deployed in the manner described in 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>Appendix A</code></b></a> 
of the <em>Hadoop/Table API Example</em>, after adding <code>kvclient.jar</code> to 
the <code>HIVE_AUX_JARS_PATH</code> environment variable of the Hive client 
(as descibed in 
<a href="#appendix_b_set_hive_aux_jars_path_nonsecure"><b><code>Appendix B</code></b></a> 
of this document), you can create the external Hive table required by this example by typing the following 
at the Hive CLI command prompt: 

<pre>
  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          vehicleTable (TYPE STRING, MAKE STRING, MODEL STRING, CLASS STRING, COLOR STRING, PRICE DOUBLE, COUNT INT) 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "vehicleTable",
                         "oracle.kv.hadoop.hosts" = "dn-host-1,dn-host-2,dn-host-3");
</pre>

The command above applies the required <a href="#ondb_hive_data_model_mapping_table"><b><code>data model mapping</code></b></a>
to create a Hive table named <em>vehicleTable</em> with columns whose types are consistent 
with the corresponding fields of the Oracle NoSQL Database table created and populated in the 
<a href="../../table/package-summary.html#schema_vehicle_table"><b><code>Hadoop/Table API Example</code></b></a>; 
which was also named <em>vehicleTable</em>. Additionally, you should note the 
following points: 

<a name="hive_external_table_creation_notes"/>
<ul>
  <li>The contents of the example command above are displayed on separate lines
       in this document <em>for readability</em>, but in practice, because the Hive command
       interpreter may have trouble handling such multi-line commands, it is 
       generally best to enter a <em>single, continuous command</em> with no 
       line breaks.
  <li>The Hive table name is <b><em>not</em></b> required to be the same as the name of the table created in Oracle NoSQL Database;
       they are the same in the above example only for convenience.
  <li>If the KVStore is configured with multiple administrative hosts, then any subset of the names of those hosts can be included in
       the value of the <code>"oracle.kv.hosts"</code> property; as long as at least one valid administrative host (and port) is included.
  <li>With respect to the <code>"oracle.kv.hadoop.hosts"</code> property in the example command above:
    <ul>
      <li>This property is currently <b><em>optional</em></b> for all systems except the 
           <a href="#oracle_bigdata_sql"><b><code>Oracle Big Data SQL</code></b></a> system.
      <li>If this property is specified on a system that does not require it, it will have no effect.
      <li>If it is specified on a system that does require it, its value must contain the names
           of <b><em>all</em></b> of the DataNodes making up the Hadoop cluster.
    </ul>
</ul>

<a name="create_hive_external_vehicle_table_secure"/>
<b><em>&mdash; Creating a Hive External Table Mapped to 'vehicleTable' in a <strong style="color: red;">Secure</strong> KVStore &mdash;</em></b>
<p>

If you will be running your Hive query against a <em>secure</em> KVStore 
deployed in the manner described in 
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>Appendix B</code></b></a> 
of the <em>Hadoop/Table API Example</em>, and if your password storage employs a 
password file instead of an Oracle Wallet, then you can create the external Hive 
table required by this example by typing the following at the Hive CLI command 
prompt: 

<pre>
  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">vehicleTablePasswd</strong> (TYPE STRING, MAKE STRING, MODEL STRING, CLASS STRING, COLOR STRING, PRICE DOUBLE, COUNT INT) 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "vehicleTable",
                         "oracle.kv.hadoop.hosts" = "dn-host-1,dn-host-2,dn-host-3",
                         <strong style="color: purple;">"oracle.kv.security" = "/tmp/hive-nosql.login"</strong>, 
                         <strong style="color: purple;">"oracle.kv.ssl.trustStore" = "/tmp/client.trust"</strong>, 
                         <strong style="color: purple;">"oracle.kv.auth.username" = "example-user"</strong>, 
                         <strong style="color: red;">"oracle.kv.auth.pwdfile.file" = "/tmp/example-user.passwd"</strong>);
</pre>

On the other hand, if your password storage employs an Oracle Wallet instead of a 
password file, then type the following at the command prompt to create the required 
Hive table,

<pre>
  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">vehicleTableWallet</strong> (TYPE STRING, MAKE STRING, MODEL STRING, CLASS STRING, COLOR STRING, PRICE DOUBLE, COUNT INT) 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "vehicleTable",
                         "oracle.kv.hadoop.hosts" = "dn-host-1,dn-host-2,dn-host-3",
                         <strong style="color: purple;">"oracle.kv.security" = "/tmp/hive-nosql.login"</strong>, 
                         <strong style="color: purple;">"oracle.kv.ssl.trustStore" = "/tmp/client.trust"</strong>, 
                         <strong style="color: purple;">"oracle.kv.auth.username" = "example-user"</strong>, 
                         <strong style="color: red;">"oracle.kv.auth.wallet.dir" = "/tmp/example-user-wallet.dir"</strong>);
</pre>

where the commands above each apply the required 
<a href="#ondb_hive_data_model_mapping_table"><b><code>data model mapping</code></b></a>
to create a Hive table named <em>vehicleTablePasswd</em> and <em>vehicleTableWallet</em> 
respectively; each with the same structure, schema, and attributes as the 
<em>vehicleTable</em> created in the non-secure case. And where the same 
<a href="#hive_external_table_creation_notes"><b><code>additional points</code></b></a>
noted for the non-secure case also apply in this case.
<p>

Note the additional properties specified in the <code>TBLPROPERTIES</code> directive; 
that is, <code>oracle.kv.security</code>, <code>oracle.kv.ssl.trustStore</code>, 
<code>oracle.kv.auth.username</code>, and <code>oracle.kv.auth.pwdfile.file</code> 
(or <code>oracle.kv.auth.wallet.dir</code>). Each of these additional properties 
corresponds to one of the artifacts required to access a secure KVStore; where 
the nature of each property and its corresponding value is described in detail in 
<a href="../../table/package-summary.html#appendix_c_secure_kvclient_packaging_model"><b><code>Appendix C</code></b></a> 
of the <em>Hadoop/Table API Example</em>.

<a name="execute_example_1_hive_query"/>
<h4>Execute Example Hive Queries Against the Oracle NoSQL Database 'vehicleTable'</h4> 

After creating and mapping the Hive tables described above to the Oracle NoSQL Database 
'vehicleTable', the data in that table can be queried in the manner shown below by querying
each Hive table. Note that because three Hive tables were created (<em>vehicleTable</em>, 
<em>vehicleTablePasswd</em>, and <em>vehicleTableWallet</em>), where the only difference 
is whether the KVStore is non-secure or secure and, if secure, whether the password is 
stored in a password file or a wallet, for each type of query that is demonstrated, three 
instances of the given example query are shown; one for each of the three Hive tables. 
Additionally, note that for any given query, the output of the query will be similar whether 
it is executed against the Hive <em>vehicleTable</em>, <em>vehicleTablePasswd</em>, 
or <em>vehicleTableWallet</em>. Therefore, to save space, for each example 
query that is presented for the three Hive tables, sample output is shown only once.
<p>

Thus, from the Hive command prompt, type the following queries for each Hive table 
and observe that the output of each query you execute, if successful, looks 
something like the corresponding sample output shown.

<p>
<a name="query_vehicle_table_select_all_rows"/>
<b><em>&mdash; List the Contents of Each Row in 'vehicleTable' &mdash;</em></b>

<pre>
  hive&gt; select * from vehicleTable;
  hive&gt; select * from vehicleTablePasswd;
  hive&gt; select * from vehicleTableWallet;

  OK
  auto&nbsp;&nbsp;Chrysler&nbsp;Imperial&nbsp;FrontWheelDrive&nbsp;white&nbsp;&nbsp;20743.943359375&nbsp;5
  auto&nbsp;&nbsp;GM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Impala&nbsp;&nbsp;&nbsp;4WheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;black&nbsp;&nbsp;20743.91015625&nbsp;&nbsp;46
  auto&nbsp;&nbsp;GM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Impala&nbsp;&nbsp;&nbsp;FrontWheelDrive&nbsp;yellow&nbsp;20743.5390625&nbsp;&nbsp;&nbsp;28
  truck&nbsp;Ford&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F250&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AllWheelDrive&nbsp;&nbsp;&nbsp;blue&nbsp;&nbsp;&nbsp;31115.759765625&nbsp;47
  ..........
</pre>

Note that unlike the more complicated example queries below, the query above 
does <b><em>not</em></b> result in the execution of a MapReduce job. 
This is because there is enough metadata in the Hive metastore to satisfy
the query.

<p>
<a name="query_vehicle_table_count_rows"/>
<b><em>&mdash; Count the Total Number of Rows in 'vehicleTable' &mdash;</em></b>

<pre>
  hive&gt; select count(type) from vehicleTable;
  hive&gt; select count(type) from vehicleTablePasswd;
  hive&gt; select count(type) from vehicleTableWallet;

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
  2014-12-12 12:04:18,403 Stage-1 map = 0%,  reduce = 0%
  2014-12-12 12:05:12,431 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 2.26 sec
  2014-12-12 12:05:13,816 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 6.7 sec
  2014-12-12 12:05:15,201 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 6.87 sec
  2014-12-12 12:05:16,594 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.16 sec
  2014-12-12 12:05:17,980 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.16 sec
  2014-12-12 12:05:19,364 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.16 sec
  2014-12-12 12:05:20,754 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.24 sec
  2014-12-12 12:05:22,140 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.24 sec
  ..........
  Job 0: Map: 6  Reduce: 1   Cumulative CPU: 15.24 sec   HDFS Read: 4532 HDFS Write: 3 SUCCESS
  Total MapReduce CPU Time Spent: 15 seconds 240 msec
  OK
  79
  Time taken: 89.359 seconds, Fetched: 1 row(s)
</pre>

<a name="query_vehicle_table_min_price"/>
<b><em>&mdash; For All Vehicles in 'vehicleTable' Find the Lowest Price &mdash;</em></b>

<pre>
  hive&gt; select min(price) from vehicleTable;
  hive&gt; select min(price) from vehicleTablePasswd;
  hive&gt; select min(price) from vehicleTableWallet;

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
  2014-12-12 12:11:10,924 Stage-1 map = 0%,  reduce = 0%
  2014-12-12 12:12:06,213 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 6.77 sec
  2014-12-12 12:12:07,606 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 6.77 sec
  2014-12-12 12:12:09,076 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.16 sec
  2014-12-12 12:12:10,464 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.3 sec
  2014-12-12 12:12:11,849 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.3 sec
  2014-12-12 12:12:13,238 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.3 sec
  2014-12-12 12:12:14,629 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.38 sec
  2014-12-12 12:12:16,031 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.38 sec
  ..........
  Job 0: Map: 6  Reduce: 1   Cumulative CPU: 15.38 sec   HDFS Read: 4532 HDFS Write: 16 SUCCESS
  Total MapReduce CPU Time Spent: 15 seconds 380 msec
  OK
  20743.244140625
  Time taken: 89.615 seconds, Fetched: 1 row(s)
</pre>

<a name="query_vehicle_table_all_gm_vehicles"/>
<b><em>&mdash; List All GM Vehicles in 'vehicleTable' &mdash;</em></b>

<pre>
  hive&gt; select * from vehicleTable where make LIKE "%GM%"; 
  hive&gt; select * from vehicleTablePasswd where make LIKE "%GM%"; 
  hive&gt; select * from vehicleTableWallet where make LIKE "%GM%"; 

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2014-12-12 12:19:24,269 Stage-1 map = 0%,  reduce = 0%
  2014-12-12 12:20:18,239 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 2.43 sec
  2014-12-12 12:20:19,622 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 4.81 sec
  2014-12-12 12:20:21,006 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 7.14 sec
  2014-12-12 12:20:22,395 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 13.09 sec
  2014-12-12 12:20:23,777 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.06 sec
  2014-12-12 12:20:25,162 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.06 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 16.06 sec   HDFS Read: 4532 HDFS Write: 1491 SUCCESS
  Total MapReduce CPU Time Spent: 16 seconds 60 msec
  OK
  suv&nbsp;&nbsp;&nbsp;GM&nbsp;Equinox&nbsp;&nbsp;4WheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yellow&nbsp;41486.78125&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;37
  truck&nbsp;GM&nbsp;Sierra&nbsp;&nbsp;&nbsp;4WheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;black&nbsp;&nbsp;31115.224609375&nbsp;87
  auto&nbsp;&nbsp;GM&nbsp;Corvette&nbsp;FrontWheelDrive&nbsp;&nbsp;yellow&nbsp;20743.84375&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7
  auto&nbsp;&nbsp;GM&nbsp;Impala&nbsp;&nbsp;&nbsp;4WheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;black&nbsp;&nbsp;20743.91015625&nbsp;&nbsp;46
  ..........
</pre>

<a name="query_vehicle_table_all_models_sierra_truck"/>
<b><em>&mdash; List All Sierra Trucks in 'vehicleTable' &mdash;</em></b>

<pre>
  hive&gt; select * from vehicleTable where model LIKE "%Sierra%"; 
  hive&gt; select * from vehicleTablePasswd where model LIKE "%Sierra%"; 
  hive&gt; select * from vehicleTableWallet where model LIKE "%Sierra%"; 

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2014-12-12 13:36:24,284 Stage-1 map = 0%,  reduce = 0%
  2014-12-12 13:37:19,528 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 9.35 sec
  2014-12-12 13:37:20,910 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 11.98 sec
  2014-12-12 13:37:22,296 Stage-1 map = 97%,  reduce = 0%, Cumulative CPU 15.31 sec
  2014-12-12 13:37:23,681 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.58 sec
  2014-12-12 13:37:25,069 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.58 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 15.58 sec   HDFS Read: 4532 HDFS Write: 496 SUCCESS
  Total MapReduce CPU Time Spent: 15 seconds 580 msec
  OK
  truck&nbsp;GM&nbsp;Silverado2500&nbsp;4WheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;blue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31115.548828125&nbsp;68
  truck&nbsp;GM&nbsp;Silverado2500&nbsp;4WheelDrive-4cylinder&nbsp;blue-on-green&nbsp;31114.91015625&nbsp;&nbsp;17
  truck&nbsp;GM&nbsp;Silverado2500&nbsp;AllWheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;white&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31115.275390625&nbsp;36
  truck&nbsp;GM&nbsp;Silverado2500&nbsp;AllWheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yellow&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31114.796875&nbsp;&nbsp;&nbsp;&nbsp;73
  truck&nbsp;GM&nbsp;Silverado1500&nbsp;4WheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;black&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31114.98046875&nbsp;&nbsp;64
  truck&nbsp;GM&nbsp;Silverado1500&nbsp;4WheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;green&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31115.078125&nbsp;&nbsp;&nbsp;&nbsp;38
  truck&nbsp;GM&nbsp;Silverado1500&nbsp;RearWheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;blue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31115.640625&nbsp;&nbsp;&nbsp;&nbsp;55
  truck&nbsp;GM&nbsp;Silverado1500&nbsp;RearWheelDrive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;white&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31115.517578125&nbsp;37
  ..........
  Time taken: 83.589 seconds, Fetched: 8 row(s)
</pre>

<a name="hive_query_example_2"/>
<h3>Example 2: Executing Hive Queries Against Non-Primitive Data Types</h3>

This second example demonstrates how to execute various Hive queries against an 
Oracle NoSQL Database table defined with a more complex schema than that employed
in <a href="#hive_query_example_1"><b><code>Example 1</code></b></a>. 
In this example, a schema is employed that consists of a variety of data types; 
both primitive and non-primitive. And in the same manner as  
<a href="#hive_query_example_1"><b><code>Example 1</code></b></a>, 
you must take the following initial steps to run the queries presented in this example:

<ul>
  <li>Satisfy the necessary <a href="#prerequisites_hive"><b><code>prerequisites</code></b></a>.
  <li>Deploy either a 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>non-secure</code></b></a>  
or a 
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>secure</code></b></a> KVStore (or both).
  <li>Follow the instructions presented in <a href="#appendix_a_create_populate_rmv_table"><b><code>Appendix A</code></b></a>
of this document to create and populate a table named <em>rmvTable</em> in each store 
that you deploy.
  <li>If any of the stores you deploy are configured for secure access, then follow the steps provided in 
<a href="#appendix_c_hive_ondb_security"><b><code>Appendix C</code></b></a> 
of this document to configure Hive with the artifacts and environment necessary to interact 
with a secure KVStore.
</ul>

After performing each of these initial steps, you can then create a Hive external table mapped to 
the KVStore <em>rmvTable</em>, and execute the set of example Hive queries (described below) 
against the data stored in that table.
<p>

<a name="create_hive_external_rmv_table"/>
<h4>Create a Hive External Table Mapped to the Oracle NoSQL Database 'rmvTable'</h4> 

After performing the initial steps listed in the previous section, use the Hive CLI  
to create a Hive external table mapped to the Oracle NoSQL Database <em>rmvTable</em> 
located in each KVStore that was deployed. That is, type the following at the command line 
of the Hive client node: 

<pre>
  &gt; hive
</pre>

which should present the following Hive CLI command prompt:

<pre>
  hive&gt;
</pre>

At this point, you can execute the Hive commands presented in the next sub-sections 
for the non-secure KVStore or the secure KVStore (or both).

<p>
<a name="create_hive_external_rmv_table_nonsecure"/>
<b><em>&mdash; Creating a Hive External Table Mapped to 'rmvTable' in a <strong style="color: green;">Non-Secure</strong> KVStore &mdash;</em></b>
<p>

If you will be running your Hive query against a <em>non-secure</em> KVStore 
deployed in the manner described in 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>Appendix A</code></b></a> 
of the <em>Hadoop/Table API Example</em>, after adding <code>kvclient.jar</code> to 
the <code>HIVE_AUX_JARS_PATH</code> environment variable of the Hive client 
(as descibed in 
<a href="#appendix_b_set_hive_aux_jars_path_nonsecure"><b><code>Appendix B</code></b></a> 
of this document), you can create the external Hive table required by this example by typing the following 
at the Hive CLI command prompt: 

<pre>
  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          rmvTable (ZIPCODE STRING, LASTNAME STRING, FIRSTNAME STRING, 
                    SSN BIGINT, 
                    GENDER STRING, 
                    LICENSE BINARY, 
                    PHONEINFO MAP&lt;STRING, STRING&gt;, 
                    ADDRESS STRUCT&lt;NUMBER:INT, STREET:STRING, UNIT:INT, CITY:STRING, STATE:STRING, ZIP:INT&gt;, 
                    VEHICLEINFO ARRAY&lt;STRUCT&lt;TYPE:STRING, MAKE:STRING, MODEL:STRING, CLASS:STRING, COLOR:STRING, VALUE:FLOAT, TAX:DOUBLE, PAID:BOOLEAN&gt;&gt;)
          COMMENT 'Hive table rmvTable &lt;---&gt; KVStore table rmvTable' 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "rmvTable",
                         "oracle.kv.hadoop.hosts" = "dn-host-1,dn-host-2,dn-host-3");
</pre>

Noting the same <a href="#hive_external_table_creation_notes"><b><code>additional points</code></b></a>
that were noted for each of the cases presented in <a href="#hive_query_example_1"><b><code>Example 1</code></b></a>, 
the command above applies the required 
<a href="#ondb_hive_data_model_mapping_table"><b><code>Oracle NoSQL Database-to-Hive Data Model Mappings</code></b></a> 
to create a Hive table named <em>rmvTable</em> with columns whose types are consistent with the corresponding 
fields of the Oracle NoSQL Database table &mdash; also named <em>rmvTable</em> &mdash; that you created 
and populated when you followed the steps outlined in 
<a href="#appendix_a_create_populate_rmv_table"><b><code>Appendix A</code></b></a> of this document.
<p>

Note that it is important to know the specific schema of the Oracle NoSQL Database table 
you wish to query before specifying the schema of that table's corresponding Hive 
external table. For example, observe that the field named <em>gender</em> in the 
<a href="#appendix_a_table_schema_rmv_table"><b><code>schema</code></b></a>
of the Oracle NoSQL Database <em>rmvTable</em> presented in Appendix C is defined as type 
<a href="../../../../javadoc/oracle/kv/table/FieldDef.Type.html#ENUM"><b><code>FieldDef.Type.ENUM</code></b></a>, 
whereas the <code>GENDER</code> field specified in the Hive external table created 
above is defined as type <code>STRING</code>; as specified by the required 
<a href="#ondb_hive_data_model_mapping_table"><b><code>data model mapping</code></b></a>.
Thus, the combination of the 
<a href="#ondb_hive_data_model_mapping_table"><b><code>Oracle NoSQL Database-to-Hive Data Model Mapping</code></b></a> 
with the given Oracle NoSQL Database table's specific <a href="#appendix_a_table_schema_rmv_table"><b><code>schema</code></b></a> 
allows you to determine whether a <code>STRING</code> field in a Hive table is mapped from an Oracle NoSQL Database 
<a href="../../../../javadoc/oracle/kv/table/FieldDef.Type.html#STRING"><b><code>FieldDef.Type.STRING</code></b></a> 
or 
<a href="../../../../javadoc/oracle/kv/table/FieldDef.Type.html#ENUM"><b><code>FieldDef.Type.ENUM</code></b></a>. 
<p>

<a name="create_hive_external_rmv_table_secure"/>
<b><em>&mdash; Creating a Hive External Table Mapped to 'rmvTable' in a <strong style="color: red;">Secure</strong> KVStore &mdash;</em></b>
<p>

If you will be running your Hive query against a <em>secure</em> KVStore 
deployed in the manner described in 
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>Appendix B</code></b></a> 
of the <em>Hadoop/Table API Example</em>, and if your password storage employs a 
password file instead of an Oracle Wallet, then you can create the external Hive 
table required by this example by typing the following at the Hive CLI command 
prompt: 

<pre>
  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">rmvTablePasswd</strong> (ZIPCODE STRING, LASTNAME STRING, FIRSTNAME STRING, 
                    SSN BIGINT, 
                    GENDER STRING, 
                    LICENSE BINARY, 
                    PHONEINFO MAP&lt;STRING, STRING&gt;, 
                    ADDRESS STRUCT&lt;NUMBER:INT, STREET:STRING, UNIT:INT, CITY:STRING, STATE:STRING, ZIP:INT&gt;, 
                    VEHICLEINFO ARRAY&lt;STRUCT&lt;TYPE:STRING, MAKE:STRING, MODEL:STRING, CLASS:STRING, COLOR:STRING, VALUE:FLOAT, TAX:DOUBLE, PAID:BOOLEAN&gt;&gt;)
          COMMENT 'Hive table rmvTablePasswd &lt;---&gt; KVStore table rmvTable' 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "rmvTable",
                         "oracle.kv.hadoop.hosts" = "dn-host-1,dn-host-2,dn-host-3",
                         <strong style="color: purple;">"oracle.kv.security" = "/tmp/hive-nosql.login"</strong>, 
                         <strong style="color: purple;">"oracle.kv.ssl.trustStore" = "/tmp/client.trust"</strong>, 
                         <strong style="color: purple;">"oracle.kv.auth.username" = "example-user"</strong>, 
                         <strong style="color: red;">"oracle.kv.auth.pwdfile.file" = "/tmp/example-user.passwd"</strong>);
</pre>

On the other hand, if your password storage employs an Oracle Wallet instead of a 
password file, then type the following at the command prompt to create the required 
Hive table,

<pre>
  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">rmvTableWallet</strong> (ZIPCODE STRING, LASTNAME STRING, FIRSTNAME STRING, 
                    SSN BIGINT, 
                    GENDER STRING, 
                    LICENSE BINARY, 
                    PHONEINFO MAP&lt;STRING, STRING&gt;, 
                    ADDRESS STRUCT&lt;NUMBER:INT, STREET:STRING, UNIT:INT, CITY:STRING, STATE:STRING, ZIP:INT&gt;, 
                    VEHICLEINFO ARRAY&lt;STRUCT&lt;TYPE:STRING, MAKE:STRING, MODEL:STRING, CLASS:STRING, COLOR:STRING, VALUE:FLOAT, TAX:DOUBLE, PAID:BOOLEAN&gt;&gt;)
          COMMENT 'Hive table rmvTableWallet &lt;---&gt; KVStore table rmvTable' 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "rmvTable",
                         "oracle.kv.hadoop.hosts" = "dn-host-1,dn-host-2,dn-host-3",
                         <strong style="color: purple;">"oracle.kv.security" = "/tmp/hive-nosql.login"</strong>, 
                         <strong style="color: purple;">"oracle.kv.ssl.trustStore" = "/tmp/client.trust"</strong>, 
                         <strong style="color: purple;">"oracle.kv.auth.username" = "example-user"</strong>, 
                         <strong style="color: red;">"oracle.kv.auth.wallet.dir" = "/tmp/example-user-wallet.dir"</strong>);
</pre>

where the commands above each combine the required 
<a href="#ondb_hive_data_model_mapping_table"><b><code>data model mapping</code></b></a>
with the <a href="#appendix_a_table_schema_rmv_table"><b><code>schema</code></b></a> of the 
<em>rmvTable</em> that you <a href="#appendix_a_create_populate_rmv_table"><b><code>created</code></b></a> 
to produce a Hive external table named <em>rmvTablePasswd</em> and <em>rmvTableWallet</em> 
respectively; each with the same structure, schema, and attributes as the Hive <em>rmvTable</em> 
produced in the non-secure case. And where the same 
<a href="#hive_external_table_creation_notes"><b><code>additional points</code></b></a>
noted for the non-secure case also apply in this case.
<p>

Note the additional properties specified in the <code>TBLPROPERTIES</code> directive; 
that is, <code>oracle.kv.security</code>, <code>oracle.kv.ssl.trustStore</code>, 
<code>oracle.kv.auth.username</code>, and <code>oracle.kv.auth.pwdfile.file</code> 
(or <code>oracle.kv.auth.wallet.dir</code>). Each of these additional properties 
corresponds to one of the artifacts required to access a secure KVStore; where 
the nature of each property and its corresponding value is described in detail in 
<a href="../../table/package-summary.html#appendix_c_secure_kvclient_packaging_model"><b><code>Appendix C</code></b></a> 
of the <em>Hadoop/Table API Example</em>.

<a name="execute_example_2_hive_query"/>
<h4>Execute Example Hive Queries Against the Oracle NoSQL Database 'rmvTable'</h4> 

After creating and mapping the Hive tables described above to the Oracle NoSQL Database 
<em>rmvTable</em>, the data in that table can be queried in the manner shown below by querying
each Hive table. Note that because three Hive tables were created (<em>rmvTable</em>, 
<em>rmvTablePasswd</em>, and <em>rmvTableWallet</em>), where the only difference 
is whether the KVStore is non-secure or secure and, if secure, whether the password is 
stored in a password file or a wallet, for each type of query that is demonstrated, three 
instances of the given example query are shown; one for each of the three Hive tables. 
Additionally, note that for any given query, the output of the query will be similar whether 
it is executed against the Hive <em>rmvTable</em>, <em>rmvTablePasswd</em>, 
or <em>rmvTableWallet</em>. Therefore, to save space, for each example 
query that is presented for the three Hive tables, sample output is shown only once.
<p>

Thus, from the Hive command prompt, type the following queries for each Hive table 
and observe that the output of each query you execute, if successful, looks 
something like the corresponding sample output shown.

<p>
<a name="query_rmv_table_select_all_rows"/>
<b><em>&mdash; List the Contents of Each Row in the 'rmvTable' &mdash;</em></b>

<pre>
  hive&gt; select * from rmvTable;
  hive&gt; select * from rmvTablePasswd;
  hive&gt; select * from rmvTableWallet;

  OK
  49027&nbsp;&nbsp;GOMEZ&nbsp;CHRISTOPHER&nbsp;&nbsp;509367447&nbsp;&nbsp;male&nbsp;&nbsp;S57428836&nbsp;&nbsp;
  {"cell":"616-351-0185","home":"213-630-2419","work":"617-227-9840"}
  {"number":88072,"street":"Fifth Avenue","unit":6,"city":"Cambridge","state":"OK","zip":49027}
  [{"type":"auto","make":"Ford","model":"Taurus","class":"AllWheelDrive","color":"blue","value":20743.234,"tax":566.290283203125,"paid":false},
   {"type":"auto","make":"Ford","model":"Taurus","class":"FrontWheelDrive","color":"blue","value":20743.559,"tax":566.2991333007812,"paid":true}]
  40719&nbsp;&nbsp;ROSARIO&nbsp;&nbsp;ANNE&nbsp;&nbsp;448406765&nbsp;&nbsp;female&nbsp;&nbsp;S04809975
  {"cell":"303-804-1660","home":"408-630-2412","work":"415-804-9515"}
  {"number":96581,"street":"Third Avenue","unit":7,"city":"Springfield","state":"RI","zip":40719}
  [{"type":"truck","make":"Chrysler","model":"Ram3500","class":"RearWheelDrive","color":"blue","value":31115.26,"tax":849.4465942382812,"paid":true},
   {"type":"truck","make":"Chrysler","model":"Ram1500","class":"AllWheelDrive","color":"blue","value":31114.873,"tax":849.43603515625,"paid":false},
   {"type":"auto","make":"Ford","model":"Edge","class":"RearWheelDrive","color":"yellow","value":20743.889,"tax":566.3081665039062,"paid":true}]
  ..........
</pre>

Note that unlike the more complicated example queries below, the query above 
does <b><em>not</em></b> result in the execution of a MapReduce job. 
This is because there is enough metadata in the Hive metastore to satisfy
the query.

<p>
<a name="query_rmv_table_list_name_gender_address"/>
<b><em>&mdash; List the Name, Gender, and Address of Each Vehicle Owner in 'rmvTable' &mdash;</em></b>

<pre>
  hive&gt; select lastname,firstname,gender,address from rmvTable;
  hive&gt; select lastname,firstname,gender,address from rmvTablePasswd;
  hive&gt; select lastname,firstname,gender,address from rmvTableWallet;

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2015-06-25 05:39:04,940 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 05:39:43,752 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 8.38 sec
  2015-06-25 05:39:45,242 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 21.73 sec
  2015-06-25 05:39:46,727 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 26.72 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 26.72 sec   HDFS Read: 4760 HDFS Write: 4702 SUCCESS
  Total MapReduce CPU Time Spent: 26 seconds 720 msec
  OK
  SNIDER&nbsp;FRANK&nbsp;&nbsp;male&nbsp;{"number":33512,"street":"Summer Street","unit":1,"city":"Arlington","state":"TN","zip":89150}
  MILLER&nbsp;ROCH&nbsp;&nbsp;male&nbsp;{"number":25698,"street":"Mullberry Street","unit":6,"city":"Madison","state":"VA","zip":5740}
  TATE&nbsp;BENJAMIN&nbsp;&nbsp;male&nbsp;{"number":2894,"street":"View Street","unit":-1,"city":"Clinton","state":"KY","zip":57466}
  ..........
  Time taken: 87.327 seconds, Fetched: 79 row(s)
</pre>

<p>
<a name="query_rmv_table_list_name_gender_address"/>
<b><em>&mdash; List the Name and Phone Number (Home, Cell, or Work) of Each Vehicle Owner in 'rmvTable' &mdash;</em></b>

<pre>
  hive&gt; select firstname,lastname,phoneinfo["home"] from rmvTable;
  hive&gt; select firstname,lastname,phoneinfo["cell"] from rmvTablePasswd;
  hive&gt; select firstname,lastname,phoneinfo["work"] from rmvTableWallet;

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2015-06-25 06:08:09,885 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 06:08:41,094 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 18.11 sec
  2015-06-25 06:08:42,582 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 18.11 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 18.11 sec   HDFS Read: 4724 HDFS Write: 2141 SUCCESS
  Total MapReduce CPU Time Spent: 18 seconds 110 msec
  OK
  CHRISTOPHER&nbsp;GOMEZ&nbsp;&nbsp;213-630-2419
  ANNE&nbsp;ROSARIO&nbsp;&nbsp;408-630-2412
  MEGAN&nbsp;PHELPS&nbsp;&nbsp;978-541-5710
  MICHAEL&nbsp;BRADLEY&nbsp;&nbsp;313-351-4580
  ..........
  Time taken: 69.173 seconds, Fetched: 79 row(s)
</pre>

<p>
<a name="query_rmv_table_count_rows"/>
<b><em>&mdash; Count the Total Number of Rows in 'rmvTable' &mdash;</em></b>

<pre>
  hive&gt; select count(vehicleinfo[0].type) from rmvTable;
  hive&gt; select count(vehicleinfo[0].type) from rmvTablePasswd;
  hive&gt; select count(vehicleinfo[0].type) from rmvTableWallet;

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
  2015-06-25 05:53:31,200 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 05:54:09,856 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 12.12 sec
  2015-06-25 05:54:11,346 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.45 sec
  2015-06-25 05:54:12,835 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.45 sec
  2015-06-25 05:54:14,336 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 25.51 sec
  2015-06-25 05:54:15,825 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 25.51 sec
  ..........
  Job 0: Map: 6  Reduce: 1   Cumulative CPU: 25.51 sec   HDFS Read: 4760 HDFS Write: 3 SUCCESS
  Total MapReduce CPU Time Spent: 25 seconds 510 msec
  OK
  79
  Time taken: 82.824 seconds, Fetched: 1 row(s)
</pre>

<p>
<a name="query_rmv_table_min_price"/>
<b><em>&mdash; For Each Owner's Primary Vehicle in 'rmvTable', Find the Minimum Assessed Value &mdash;</em></b>

<pre>
  hive&gt; select min(vehicleinfo[0].value) from rmvTable;
  hive&gt; select min(vehicleinfo[0].value) from rmvTablePasswd;
  hive&gt; select min(vehicleinfo[0].value) from rmvTableWallet;

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
  2014-12-12 12:11:10,924 Stage-1 map = 0%,  reduce = 0%
  2014-12-12 12:12:06,213 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 6.77 sec
  2014-12-12 12:12:07,606 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 6.77 sec
  2014-12-12 12:12:09,076 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.16 sec
  2014-12-12 12:12:10,464 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.3 sec
  2014-12-12 12:12:11,849 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.3 sec
  2014-12-12 12:12:13,238 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.3 sec
  2014-12-12 12:12:14,629 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.38 sec
  2014-12-12 12:12:16,031 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.38 sec
  ..........
  Job 0: Map: 6  Reduce: 1   Cumulative CPU: 15.38 sec   HDFS Read: 4532 HDFS Write: 16 SUCCESS
  Total MapReduce CPU Time Spent: 15 seconds 380 msec
  OK
  20743.244140625
  Time taken: 89.615 seconds, Fetched: 1 row(s)
</pre>

<p>
<a name="query_rmv_table_vehicle_info"/>
<b><em>&mdash; List all Vehicle Information for the Primary (or Second or Third) Vehicle of Each Owner in 'rmvTable' &mdash;</em></b>

<pre>
  hive&gt; select vehicleinfo[0] from rmvTable;
  hive&gt; select vehicleinfo[1] from rmvTablePasswd;
  hive&gt; select vehicleinfo[2] from rmvTableWallet;

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2015-06-25 06:31:54,779 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 06:32:33,430 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 4.59 sec
  2015-06-25 06:32:34,919 Stage-1 map = 95%,  reduce = 0%, Cumulative CPU 27.33 sec
  2015-06-25 06:32:36,416 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.89 sec
  2015-06-25 06:32:37,903 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.89 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 27.89 sec   HDFS Read: 4760 HDFS Write: 5681 SUCCESS
  Total MapReduce CPU Time Spent: 27 seconds 890 msec
  OK
  {"type":"suv","make":"GM","model":"Tahoe","class":"4WheelDrive","color":"black","value":41487.242,"tax":1132.6016845703125,"paid":true}
  {"type":"auto","make":"Chrysler","model":"Imperial","class":"AllWheelDrive","color":"red","value":20743.926,"tax":566.3092041015625,"paid":true}
  {"type":"auto","make":"Ford","model":"Taurus","class":"RearWheelDrive","color":"blue","value":20744.076,"tax":566.3132934570312,"paid":true}
  {"type":"truck","make":"Ford","model":"F150","class":"AllWheelDrive","color":"green","value":31115.299,"tax":849.4476928710938,"paid":false}
  ..........
  Time taken: 82.056 seconds, Fetched: 79 row(s)
</pre>

<p>
<a name="query_rmv_table_last_name_starts_with_h"/>
<b><em>&mdash; List the Name, Address, and Primary Vehicle Information of All Owners in 'rmvTable' Whose Last Name Starts with 'H' &mdash;</em></b>

<pre>
  hive&gt; select firstname,lastname,address,vehicleinfo[0] from rmvTable where lastname RLIKE "^[H].*";
  hive&gt; select firstname,lastname,address,vehicleinfo[0] from rmvTablePasswd where lastname RLIKE "^[H].*";
  hive&gt; select firstname,lastname,address,vehicleinfo[0] from rmvTableWallet where lastname RLIKE "^[H].*";

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2015-06-25 06:41:35,578 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 06:42:14,310 Stage-1 map = 15%,  reduce = 0%
  2015-06-25 06:42:15,803 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 9.46 sec
  2015-06-25 06:42:17,290 Stage-1 map = 83%,  reduce = 0%, Cumulative CPU 23.29 sec
  2015-06-25 06:42:18,778 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.78 sec
  2015-06-25 06:42:20,271 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.78 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 27.78 sec   HDFS Read: 4760 HDFS Write: 1143 SUCCESS
  Total MapReduce CPU Time Spent: 27 seconds 780 msec
  OK
  CINDY&nbsp;HODGES&nbsp;{"number":56758,"street":"Vaughan Avenue","unit":-1,"city":"Madison","state":"NH","zip":79623}
  {"type":"truck","make":"Chrysler","model":"Ram1500","class":"RearWheelDrive","color":"black","value":31115.129,"tax":849.4430541992188,"paid":true}
  JULIA&nbsp;HOLDEN&nbsp;{"number":56209,"street":"Main Street","unit":1,"city":"Georgetown","state":"CA","zip":62154}
  {"type":"auto","make":"Ford","model":"Taurus","class":"FrontWheelDrive","color":"blue","value":20743.8,"tax":566.3057861328125,"paid":true}
  PHYLLIS&nbsp;HOGAN&nbsp;{"number":47358,"street":"Park Street","unit":6,"city":"Arlington","state":"AL","zip":77333}
  {"type":"suv","make":"Chrysler","model":"Journey","class":"RearWheelDrive","color":"yellow","value":41486.79,"tax":1132.58935546875,"paid":false}
  ..........
  Time taken: 81.171 seconds, Fetched: 9 row(s)
</pre>

<p>
<a name="query_rmv_table_second_vehicle_gm"/>
<b><em>&mdash; List the Name, Address, and Vehicle Information of Each Owner in 'rmvTable' Whose Second Vehicle is a GM Vehicle &mdash;</em></b>

<pre>
  hive&gt; select firstname,lastname,address,vehicleinfo[1] from rmvTable where vehicleinfo[1].make LIKE "%GM%";
  hive&gt; select firstname,lastname,address,vehicleinfo[1] from rmvTablePasswd where vehicleinfo[1].make LIKE "%GM%";
  hive&gt; select firstname,lastname,address,vehicleinfo[1] from rmvTableWallet where vehicleinfo[1].make LIKE "%GM%";

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2015-06-25 06:57:01,117 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 06:57:32,506 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 9.29 sec
  2015-06-25 06:57:34,000 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 18.8 sec
  2015-06-25 06:57:35,484 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 18.8 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 18.8 sec   HDFS Read: 4724 HDFS Write: 2087 SUCCESS
  Total MapReduce CPU Time Spent: 18 seconds 800 msec
  OK
  NANCY&nbsp;STOUT&nbsp;{"number":31126,"street":"Cedar Street","unit":8,"city":"Arlington","state":"MO","zip":731}
  {"type":"suv","make":"GM","model":"Equinox","class":"AllWheelDrive","color":"red","value":41486.43,"tax":1132.57958984375,"paid":true}
  DONALD&nbsp;COOK&nbsp;{"number":34287,"street":"Third Avenue","unit":1,"city":"Cambridge","state":"NV","zip":67995}
  {"type":"suv","make":"GM","model":"Tahoe","class":"AllWheelDrive","color":"black","value":41487.168,"tax":1132.5997314453125,"paid":false}
  RANDY&nbsp;MCDOWELL&nbsp;{"number":18391,"street":"Lane Avenue","unit":8,"city":"Concord","state":"NH","zip":42540}
  {"type":"auto","make":"GM","model":"Corvette","class":"FrontWheelDrive","color":"black","value":20744.035,"tax":566.3121337890625,"paid":false}
  ..........
  Time taken: 71.794 seconds, Fetched: 17 row(s)
</pre>

<p>
<a name="query_rmv_table_primary_vehicle_silverado"/>
<b><em>&mdash; List the Name, Address, and Vehicle Information of Each Owner in 'rmvTable' Whose Primary Vehicle is a Silverado Truck &mdash;</em></b>

<pre>
  hive&gt; select firstname,lastname,address,vehicleinfo[0] from rmvTable where vehicleinfo[0].model LIKE "%Silverado%";
  hive&gt; select firstname,lastname,address,vehicleinfo[0] from rmvTablePasswd where vehicleinfo[0].model LIKE "%Silverado%";
  hive&gt; select firstname,lastname,address,vehicleinfo[0] from rmvTableWallet where vehicleinfo[0].model LIKE "%Silverado%";

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2015-06-25 07:05:40,301 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 07:06:18,907 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 9.6 sec
  2015-06-25 07:06:20,406 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.65 sec
  2015-06-25 07:06:21,894 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.65 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 27.65 sec   HDFS Read: 4760 HDFS Write: 1185 SUCCESS
  Total MapReduce CPU Time Spent: 27 seconds 650 msec
  OK
  PHYLLIS&nbsp;MARTINEZ&nbsp;{"number":29978,"street":"Park Street","unit":7,"city":"Burlington","state":"NM","zip":28564}
  {"type":"truck","make":"GM","model":"Silverado2500","class":"4WheelDrive","color":"black","value":31115.295,"tax":849.4475708007812,"paid":false}
  SHANNON&nbsp;KELLEY&nbsp;{"number":98370,"street":"Maple Street","unit":9,"city":"Boston","state":"TX","zip":16166}
  {"type":"truck","make":"GM","model":"Silverado2500","class":"FrontWheelDrive","color":"yellow","value":31115.209,"tax":849.4451904296875,"paid":true}
  FRANCES&nbsp;HOLLOWAY&nbsp;{"number":2650,"street":"Second Avenue","unit":6,"city":"Arlington","state":"KY","zip":7182}
  {"type":"truck","make":"GM","model":"Silverado2500","class":"FrontWheelDrive","color":"yellow","value":31115.31,"tax":849.447998046875,"paid":true}
  ..........
  Time taken: 79.539 seconds, Fetched: 9 row(s)
</pre>

<p>
<a name="query_rmv_table_complex_chrysler"/>
<b><em>&mdash; List the Name, Address, Model, Assessed Value and Registration Fee (Paid or Not) for Each Chrysler Primary Vehicle in 'rmvTable' &mdash;</em></b>

<pre>
  hive&gt; select firstname,lastname,address,vehicleinfo[0].model,vehicleinfo[0].value,vehicleinfo[0].tax,vehicleinfo[0].paid from rmvTable where vehicleinfo[0].make LIKE "%Chrysler%";
  hive&gt; select firstname,lastname,address,vehicleinfo[0].model,vehicleinfo[0].value,vehicleinfo[0].tax,vehicleinfo[0].paid from rmvTablePasswd where vehicleinfo[0].make LIKE "%Chrysler%";
  hive&gt; select firstname,lastname,address,vehicleinfo[0].model,vehicleinfo[0].value,vehicleinfo[0].tax,vehicleinfo[0].paid from rmvTableWallet where vehicleinfo[0].make LIKE "%Chrysler%";

  Launching Job 1 out of 1
  ..........
  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 0
  2015-06-25 07:19:55,601 Stage-1 map = 0%,  reduce = 0%
  2015-06-25 07:20:26,973 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 18.15 sec
  2015-06-25 07:20:28,454 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 18.15 sec
  ..........
  Job 0: Map: 6   Cumulative CPU: 18.15 sec   HDFS Read: 4724 HDFS Write: 2164 SUCCESS
  Total MapReduce CPU Time Spent: 18 seconds 150 msec
  OK
  ANNE&nbsp;ROSARIO&nbsp;{"number":96581,"street":"Third Avenue","unit":7,"city":"Springfield","state":"RI","zip":40719}
  Ram3500&nbsp;31115.26&nbsp;849.4465942382812&nbsp;true
  MEGAN&nbsp;PHELPS&nbsp;{"number":12713,"street":"MAC Avenue","unit":4,"city":"Salem","state":"MS","zip":76554}
  Ram1500&nbsp;31115.309&nbsp;849.4479370117188&nbsp;true
  BRIAN&nbsp;ROWLAND&nbsp;{"number":37868,"street":"First Street","unit":3,"city":"Salem","state":"GA","zip":98106}
  Imperial&nbsp;20744.156&nbsp;566.3154907226562&nbsp;true
  ..........
  Time taken: 72.163 seconds, Fetched: 23 row(s)
</pre>

<a name="oracle_bigdata_sql"/>
<h3>Example 3: Executing Big Data SQL Queries</h3>

<a href="http://www.oracle.com/us/products/database/big-data-sql/overview/index.html"><b><code>Oracle Big Data SQL</code></b></a>
(for the purposes of this document, referred to as <em>Big Data SQL</em>) 
is a system consisting of properietary hardware and software; including an 
<a href="http://www.oracle.com/technetwork/database/bigdata-appliance/overview/index.html"><b><code>Oracle Big Data Appliance</code></b></a>, 
<a href="http://www.oracle.com/engineered-systems/exadata/index.html"><b><code>Oracle Exadata Database</code></b></a>, 
custom software extended to support <em>Big Data SQL</em> functionality, and of course, 
<a href="http://www.oracle.com/us/products/database/nosql/overview/index.html"><b><code>Oracle NoSQL Database</code></b></a> 
itself. The goal of the <em>Big Data SQL</em> product is to allow users to employ 
SQL to manage and manipulate data stored in a number of different locations. 
Specifically, <em>Big Data SQL</em> is designed to provide SQL access to 
data stored in Hadoop HDFS, various NoSQL databases 
&mdash; including <em>Oracle NoSQL Database</em> &mdash; as well as various 
relational databases. <em>Big Data SQL</em> achieves this by presenting Hadoop HDFS, 
<em>Oracle NoSQL Database</em>, and other NoSQL data sources as enhanced 
<a href="http://docs.oracle.com/cd/B19306_01/server.102/b14215/et_concepts.htm"><b><code>Oracle External Tables</code></b></a> 
(<em>external tables</em>) of the Oracle Relational Database Management System 
(<em>RDBMS</em>); mapping the external semantics of accessing data from those 
sources &mdash; horizontal parallelism, location, and schema &mdash; to 
Oracle RDBMS internals.
<p>

Because you may not have access to an <em>Oracle Big Data SQL</em> system, the 
<a href="http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html"><b><code>Oracle Big Data Lite Virtual Machine</code></b></a> 
(referred to in this document as the <em>Big Data Lite VM</em>), is provided to 
allow you to evaluate and experiment with <em>Big Data SQL</em> without requiring 
you to purchase, install, and manage the real system. Although detailed documentation 
is provided when you purchase a <em>Big Data SQL</em> system, as a convenience, this 
section walks you through an example that employs the <em>Big Data Lite VM</em> to 
demonstrate how to use <em>Big Data SQL</em> to query data stored in a KVStore 
&mdash; both secure and non-secure. After working with this example on the 
<em>Big Data Lite VM</em>, you should be able to extend the concepts that are 
demonstrated, and apply that knowledge should you ever have access to a real 
<em>Big Data SQL</em> system. 

<a name="primer_bigdata_sql"/>
<h4>A Brief <em>Big Data SQL</em> Primer</h4>

As stated above, <em>Big Data SQL</em> allows SQL access to various external data 
sources &mdash; specifically, a KVStore &mdash; by presenting such data sources as 
<a href="http://docs.oracle.com/cd/B19306_01/server.102/b14215/et_concepts.htm"><b><code>Oracle External Tables</code></b></a>. 
To achieve this, a mechanism referred to as an <em>access driver</em> is employed 
to access data as if it were a table in the Oracle relational database provided in 
the system. <em>Big Data SQL</em> extends the access driver mechanism of external 
tables by specifying new access driver types; one for each type of external data 
source that will be accessed. Prior to the introduction of <em>Big Data SQL</em>, 
the Oracle RDBMS external tables mechanism defined only two access driver types:

<ul>
  <li><b>ORACLE_LOADER</b>: for reading data from flat files.
  <li><b>ORACLE_DATAPUMP</b>: for migrating data between Oracle databases in a proprietary format.
</ul>

With the introduction of <em>Big Data SQL</em>, the following new access driver
types are defined:

<ul>
  <li><b>ORACLE_HDFS</b>: for accessing data stored in the Hadoop Distributed File System (HDFS).
  <li><b>ORACLE_HIVE</b>: for accessing data stored in Hive tables or Oracle NoSQL Database tables.
  <li><b>ORACLE_BIGDATA</b>: for providing extensibility to access other data stores.
</ul>

Both the <code>ORACLE_HDFS</code> and <code>ORACLE_HIVE</code> access drivers 
require the specification of a number of classes that satisfy the Hadoop MapReduce 
programming model. Some of those classes are required by both access driver types, 
whereas some are required by only the <code>ORACLE_HIVE</code> access driver. The 
class types required by both <code>ORACLE_HDFS</code> and <code>ORACLE_HIVE</code> 
are the same as those described in the 
<a href="../../table/package-summary.html#primer_hadoop_classes"><b><code>Hadoop/Table API Example</code></b></a>; 
whereas the class types required by <code>ORACLE_HIVE</code> but not by 
<code>ORACLE_HDFS</code> are:

<ul>
  <li>An instance of 
       <a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.html"><b><code>org.apache.hadoop.hive.ql.metadata.HiveStorageHandler</code></b></a>; for example, the Oracle NoSQL Database 
<a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableStorageHandler.html"><b><code>TableStorageHandler</code></b></a>. 
  <li>An instance of 
       <a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/serde2/SerDe.html"><b><code>org.apache.hadoop.hive.serde2.SerDe</code></b></a>; for example, the Oracle NoSQL Database 
<a href="../../../../javadoc/oracle/kv/hadoop/hive/table/TableSerDe.html"><b><code>TableSerDe</code></b></a>. 
  <li>Various instances of 
       <a href="http://hive.apache.org/javadocs/r0.12.0/api/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspector.html"><b><code>org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector</code></b></a>; 
       for example, instances of the Oracle NoSQL Database classes 
       <a href="#hive_object_inspector_impls"><b><code>described previously</code></b></a> in this document. 
</ul>

Note that the <code>ORACLE_HDFS</code> access driver can only read data stored in 
HDFS files, whereas the <code>ORACLE_HIVE</code> access driver can read data stored 
not only in HDFS files, but in other locations as well; for example, a KVStore.
As you will see below, the integration of <em>Oracle NoSQL Database</em> with Hive 
&mdash; as described <a href="#introduction_hive"><b><code>above</code></b></a> &mdash;
plays a prominent role in the integration of <em>Oracle NoSQL Database</em> with 
<em>Big Data SQL</em>. 

<a name="mapping_ora_to_ondb_data_model"/>
<h3>Mapping the Oracle RDBMS Data Model to the Oracle NoSQL Database Table API Data Model</h3>

As the examples in this section demonstrate, in order to execute a Big Data SQL query 
against data stored in an Oracle NoSQL Database table, a Hive <em>external table</em>
must first be created with a schema mapped from the schema of the desired Oracle NoSQL Database 
table, and then a corresponding Oracle RDBMS <em>external table</em> must be created with a schema 
mapped from the schema of the Hive table. This is accomplished by applying the mappings 
shown in the following table:
<p>

<a name="ondb_hive_ora_data_model_mapping_table"/>
<table border="1" style="width:50%;border-collapse:collapse">
  <caption>Oracle NoSQL Database-to-Hive-to-RDBMS Data Type Mappings</caption>
  <tr>
    <th>Oracle NoSQL Database Table API</th>
    <th>Hive</th>
    <th>Oracle RDBMS</th>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.STRING</td>
    <td align="center">STRING</td>
    <td align="center">VARCHAR2(N)</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">VARCHAR</td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">CHAR</td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.BOOLEAN</td>
    <td align="center">BOOLEAN</td>
    <td align="center">VARCHAR2(5)</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.BINARY</td>
    <td align="center">BINARY</td>
    <td align="center">VARCHAR2(N)</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.FIXED_BINARY</td>
    <td align="center">BINARY</td>
    <td align="center">VARCHAR2(N)</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">TINYINT</td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">SMALLINT</td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.INTEGER</td>
    <td align="center">INT</td>
    <td align="center">NUMBER</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.LONG</td>
    <td align="center">BIGINT</td>
    <td align="center">NUMBER</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.FLOAT</td>
    <td align="center">FLOAT</td>
    <td align="center">NUMBER</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">DECIMAL</td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.DOUBLE</td>
    <td align="center">DOUBLE</td>
    <td align="center">NUMBER</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.ENUM</td>
    <td align="center">STRING</td>
    <td align="center">VARCHAR2(N)</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">TIMESTAMP</td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">DATE</td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.ARRAY</td>
    <td align="center">ARRAY</td>
    <td align="center">VARCHAR2(N)</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.MAP</td>
    <td align="center">MAP&lt;STRING, data_type&gt;</td>
    <td align="center">VARCHAR2(N)</td>
  </tr>
  <tr>
    <td align="center">FieldDef.Type.RECORD</td>
    <td align="center">STRUCT&lt;col_name : data_type, ...&gt;</td>
    <td align="center">VARCHAR2(N)</td>
  </tr>
  <tr>
    <td align="center"></td>
    <td align="center">UNIONTYPE&lt;data_type, data_type, ...&gt;</td>
    <td align="center"></td>
  </tr>
</table>
<p>

It is important to understand that when using Big Data SQL to query data in an 
Oracle NoSQL Database table, the schema of the Oracle external table you 
create is dependent on the schema of the corresponding Hive external table; 
which, in turn, is dependent on the schema of the Oracle NoSQL Database 
table you wish to query. Thus, if either type of external table is created using 
a schema that includes a data type that does not belong to one of the end-to-end
mappings presented above, then an error will occur when any attempt is made to 
query the table.
<p>

Note that for fields in the Oracle external table specified as <code>VARCHAR2(N)</code>, 
the value of <code>N</code> is the maximum number of characters of the variable length 
<code>STRING</code> that represents the specified field in the corresponding Hive and 
Oracle NoSQL Database tables. Therefore, you should use the type, structure, and expected 
length or size of the corresponding Hive and Oracle NoSQL Database fields to determine the 
appropriate value to specify for <code>N</code> when creating the Oracle external table.

<a name="install_big_data_lite_vm"/>
<h4>Installing Big Data Lite VM</h4>

Before proceeding with this example you should obtain, install, and deploy 
the <em>Big Data Lite VM</em> as described 
<a href="http://www.oracle.com/technetwork/database/bigdata-appliance/bigdatalite-quickdeploy-241-2106479.pdf"><b><code>here</code></b></a>. Note that the <em>Big Data Lite VM</em> must be deployed 
to a virtualization product such as  
<a href="http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html"><b><code>Oracle VirtualBox</code></b></a>. 
After installing the necessary environment and deploying the <em>Big Data Lite VM</em>,
you should verify that the necessary services are running in that VM and start those
that are not running. You can do this by executing the <code>services</code> utility 
from the command line; which will display an interactive user interface with information 
like that shown below. That is, 

<pre>
  &gt; services
</pre>

<a name="bigdata_lite_vm_services_ui"/>
<table border="1" style="width:50%;border-collapse:collapse">
  <caption>Use arrow keys to navigate, space bar to select/deselect services to start/stop, enter key to save changes</caption>
  <tr>
    <td>[*]</td> <td>ORCL</td> <td>Oracle Database 12c (on)</td>
  </tr>
  <tr>
    <td>[*]</td> <td>Zookeeper</td> <td>zookeeper (on)</td>
  </tr>
  <tr>
    <td>[*]</td> <td>HDFS</td> <td>namenode (on) datanode (on) ...</td>
  </tr>
  <tr>
    <td>[*]</td> <td>Hive</td> <td>metastore (on) hive-server2 (on)</td>
  </tr>
  <tr>
    <td>[]</td> <td>Hue</td> <td>Hue (off)</td>
  </tr>
  <tr>
    <td>[]</td> <td>Impala</td> <td>impala-server (off)</td>
  </tr>
  <tr>
    <td>[]</td> <td>NoSQL</td> <td>Oracle NoSQL Database (off)</td>
  </tr>
  <tr>
    <td>[]</td> <td>Oozie</td> <td>oozie (of)</td>
  </tr>
  <tr>
    <td>[]</td> <td>Solar</td> <td>solar-server (off)</td>
  </tr>
  <tr>
    <td>[]</td> <td>SpatialAndGraph</td> <td>web-server (off)</td>
  </tr>
  <tr>
    <td>[]</td> <td>Sqoop2</td> <td>sqoop2-server (off)</td>
  </tr>
  <tr>
    <td>[]</td> <td>WebLogic-MovieDemo</td> <td>WebLogic-MovieDemo (off)</td>
  </tr>
  <tr>
    <td>[*]</td> <td>YARN</td> <td>resourcemanager (on) nodemanager (on) ...</td>
  </tr>
</table>
<p>

The services selected (marked with asterisks) are the services that are necessary
when walking through the examples presented in the following sections. In an effort 
to improve boot performance, the Big Data Lite VM will typically come out of the 
box running only the <code>Zookeeper</code>, <code>HDFS</code>, and <code>YARN</code>
services. As a result, you will need to select the <code>ORCL</code> and <code>Hive</code> 
services for auto-start. Additionally, because the examples presented here employ an 
externally deployed KVStore, there is no need to select the <code>Oracle NoSQL Dabase</code> 
services for auto-start.
<p>

The examples presented here assume that you have satisfied the 
<a href="#prerequisites_hive"><b><code>prerequisites</code></b></a> listed 
previously in this document; which means that a KVStore
&mdash; <a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>non-secure</code></b></a>,
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>secure</code></b></a>, or both &mdash;
has been deployed to separate, external nodes unrelated to the <em>Big Data Lite VM</em>;
where the nodes are network reachable from that VM. Although it is possible to select the 
<code>Oracle NoSQL Database</code> service and deploy a KVStore 
(with <code>KVHOME=/u01/nosql/kv-ee</code>) on the Big Data Lite VM, it is 
important to understand that typical deployments of the 
<a href="http://www.oracle.com/technetwork/database/bigdata-appliance/overview/index.html"><b><code>Oracle Big Data Appliance</code></b></a>
do not run the Hadoop services (<code>HDFS</code>, <code>YARN</code>, and <code>Hive</code>) 
on the same nodes used to run the <code>Oracle NoSQL Database</code> services. As a result,
the examples presented here are run in a manner intended to be consistent with the more
typical deployment scenarios.
<p>

After satisfying the necessary <a href="#prerequisites_hive"><b><code>prerequisites</code></b></a>,
creating and populating a 
<a href="../../table/package-summary.html#create_populate_vehicle_table"><b><code>vehicleTable</code></b></a>
and <a href="#appendix_a_create_populate_rmv_table"><b><code>rmvTable</code></b></a> in a KVStore,
installing and deploying the <em>Big Data Lite VM</em> and <a href="#bigdata_lite_vm_services_ui"><b><code>verifying</code></b></a> 
that the necessary services are running on that VM, you are then ready to walk through the 
<em>Big Data SQL</em> examples presented in the following sections.
<p>

<a name="big_data_sql_example_walkthrough"/>
<h4>Executing a Big Data SQL Query Against a KVStore Table</h4>

Once the KVStore 
&mdash; 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>non-secure</code></b></a> 
or 
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>secure</code></b></a> 
&mdash; 
is deployed with the expected tables, you can follow the steps presented in the next section to 
configure the <em>Big Data Lite VM</em> environment for executing either Hive or 
<em>Big Data SQL</em> queries against the data in those tables. But before proceeding 
to that section, you should first become familiar with the directory structure and base 
configuration of the <em>Big Data SQL</em> system; specifically, the following 
directories and files:

<pre>
/etc/hive/conf.bigdatalite
  <strong style="color: red;">hive-env.sh</strong>
  hive-site.xml
/u01
  /bigdatasql_config
    <strong style="color: red;">bigdata.properties</strong>
    bigdata-log4j.properties
    /bigdatalite
      core-site.xml
      hdfs-site.xml
      <strong style="color: red;">hive-env.sh</strong>
      hive-site.xml
      mapred-site.xml
    /hive_aux_jars
      apache-xmlbeans.jar -&gt; /u01/connectors/oxh/hive/lib/apache-xmlbeans.jar
      hive-hcatalog-core.jar -&gt; /usr/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core.jar
      orai18n-collation.jar -&gt; /u01/connectors/oxh/hive/lib/orai18n-collation.jar
      orai18n.jar -&gt; /u01/connectors/oxh/hive/lib/orai18n.jar
      orai18n-mapping.jar -&gt; /u01/connectors/oxh/hive/lib/orai18n-mapping.jar
      orai18n-utility.jar -&gt; /u01/connectors/oxh/hive/lib/orai18n-utility.jar
      oxh-hive.jar -&gt; /u01/connectors/oxh/hive/lib/oxh-hive.jar
      oxh-mapreduce.jar -&gt; /u01/connectors/oxh/hive/lib/oxh-mapreduce.jar
      oxquery.jar -&gt; /u01/connectors/oxh/hive/lib/oxquery.jar
      stax2-api-3.1.1.jar -&gt; /u01/connectors/oxh/hive/lib/stax2-api-3.1.1.jar
      woodstox-core-asl-4.2.0.jar -&gt; /u01/connectors/oxh/hive/lib/woodstox-core-asl-4.2.0.jar
      xmlparserv2_sans_jaxp_services.jar -&gt; /u01/connectors/oxh/hive/lib/xmlparserv2_sans_jaxp_services.jar
      xqjapi.jar -&gt; /u01/connectors/oxh/hive/lib/xqjapi.jar
      /log
        ....
  /nosql
    kv-3.4.3
    <strong style="color: red;">kv-ee -&gt; kv-3.4.3</strong>
    /scripts
      ....
</pre>

You should also note that, as described in the 
<a href="http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html"><b><code>Big Data Lite VM documentation</code></b></a>, the username <em>oracle</em> with password 
<em>welcome1</em> must be used when logging into the <em>Big Data Lite VM</em>.

<p>
<a name="config_big_data_lite_vm"/>
<b><em>&mdash; Configuring the Big Data Lite VM for Hive and Big Data SQL Queries &mdash;</em></b>
<p>

Depending on the particular version of the <em>Big Data Lite VM</em> (or the
<em>Big Data SQL</em> system) you are using, you may need to upgrade the version 
of <em>Oracle NoSQL Database</em> that is installed on the system; as well as add 
one or more JAR files to the <code>/u01/bigdatasql_config/hive_aux_jars</code> 
directory. That is, <code>/u01/nosql/kv-ee</code> should be linked to a version 
of <em>Oracle NoSQL Database</em> that is <b><em>greater than or equal to</em></b> 
the <code>3.4.3</code> version; if not, then you should install a version that 
satisfies that criterion.
<p>

With respect to the contents of <code>/u01/bigdatasql_config/hive_aux_jars</code>, 
if any of the JAR files referenced below are not contained in that directory, then 
execute the following at the command line as appropriate: 

<pre>
  &gt; cd /u01/bigdatasql_config/hive_aux_jars

  &gt; ln -s /u01/nosql/kv-ee/lib/kvclient.jar kvclient.jar
  &gt; ln -s /u01/nosql/kv-ee/lib/oraclepki.jar.jar oraclepki.jar.jar
  &gt; ln -s /u01/nosql/kv-ee/lib/osdt_cert.jar osdt_cert.jar
  &gt; ln -s /u01/nosql/kv-ee/lib/osdt_core.jar osdt_core.jar
</pre>

Next, to support running Hive (and ultimately <em>Big Data SQL</em>) queries 
against table data stored in a <em>secure</em> KVStore, the  
<a href="#appendix_c_generate_security_artifacts"><b><code>appropriate security artifacts</code></b></a>, 
along with a 
<a href="#appendix_c_generate_server_side_jar"><b><code>server side JAR file</code></b></a>
containing the necessary public credentials must be installed on the <em>Big Data Lite VM</em>. 
This can be done by either copying the artifacts already 
<a href="#appendix_c_hive_ondb_security"><b><code>generated</code></b></a> 
for the Hive example <a href="#hive_query_example_2"><b><code>described above</code></b></a>, 
or by regenerating the necessary artifacts on the <em>Big Data Lite VM</em> itself.
Either way, create the directory <code>/u01/nosql/example-user/security</code> 
and place the artifacts in that location; that is, 

<pre>
  /u01/nosql/example-user/security
    client.trust
    hive-nosql.login
    hive-nosql-server.jar
    example-user.passwd
    /example-user-wallet.dir
      cwallet.sso
</pre>

In addition to installing the above artifacts under <code>/u01/nosql/example-user/securtiy</code>,
you must also add the necessary JAR files to the <code>HIVE_AUX_JARS_PATH</code> environment 
variable. To do this, first install the <code>hive-nosql-server.jar</code> file in 
<code>/u01/bigdatasql_config/hive_aux_jars</code>; that is,

<pre>
  &gt; cd /u01/bigdatasql_config/hive_aux_jars
  &gt; ln -s /u01/nosql/example-user/security/hive-nosql-server.jar hive-nosql-server.jar
</pre>

Then add the necessary JAR files to the value of the <code>HIVE_AUX_JARS_PATH</code> 
variable set in <code>hive-env.sh</code>. The particular instance of the 
<code>hive-env.sh</code> script that you should modify is dependent on the version 
of the Big Data Lite VM that you have installed. Modify <code>/etc/hive/conf.bigdatalite/hive-env.sh</code> 
if the version is <code>4.2</code> or greater; otherwise, modify 
<code>/u01/bigdatasql_config/bigdatalite/hive-env.sh</code>. Thus, 
after changing to the appropriate directory, modify <code>hive-env.sh</code> in the 
following way,

<pre>
  &gt; edit hive-env.sh

    HIVE_AUX_JARS_PATH=/u01/bigdatasql_config/hive_aux_jars/apache-xmlbeans.jar,\
                       /u01/bigdatasql_config/hive_aux_jars/hive-hcatalog-core.jar,\
                       ....
                       <strong style="color: red;">/u01/bigdatasql_config/hive_aux_jars/kvclient.jar,\
                       /u01/bigdatasql_config/hive_aux_jars/oraclepki.jar,\
                       /u01/bigdatasql_config/hive_aux_jars/osdt_cert.jar,\
                       /u01/bigdatasql_config/hive_aux_jars/osdt_core.jar,\</strong>
                       <strong style="color: purple;">/u01/bigdatasql_config/hive_aux_jars/hive-nosql-server.jar,</strong>\
                       ....
                       /u01/bigdatasql_config/hive_aux_jars/xqjapi.jar                       
</pre>

Finally, add the necessary JAR files to the value of the <code>java.classpath.hive</code> 
variable set in <code>/u01/bigdatasql_config/bigdata.properties</code>; that is,

<pre>
  &gt; cd /u01/bigdatasql_config
  &gt; edit bigdata.properties

    java.classpath.hive=/usr/lib/hive/lib/*:\
                        /u01/bigdatasql_config/hive_aux_jars/hive-catalog-core.jar:\
                        /u01/bigdatasql_config/hive_aux_jars/hive-hcatalog-core.jar:\
                        ....
                        <strong style="color: red;">/u01/bigdatasql_config/hive_aux_jars/kvclient.jar:\
                        /u01/bigdatasql_config/hive_aux_jars/oraclepki.jar:\
                        /u01/bigdatasql_config/hive_aux_jars/osdt_cert.jar:\
                        /u01/bigdatasql_config/hive_aux_jars/osdt_core.jar:\</strong>
                        <strong style="color: purple;">/u01/bigdatasql_config/hive_aux_jars/hive-nosql-server.jar</strong>       
</pre>

Future versions of the Big Data Lite VM 
may provide versions of <code>hive-env.sh</code> and <code>bigdata.properties</code> 
that already contain the necessary references to <code>kvclient.jar</code>, 
<code>oraclepki.jar</code>, <code>osdt_cert.jar</code>, and <code>osdt_core.jar</code> 
described above. If so, then all you need to add to <code>hive-env.sh</code> and <code>bigdata.properties</code> is the reference to 
<code>hive-nosql-server.jar</code>; which is only necessary if you wish to run the examples 
presented below against a <em>secure</em> KVStore.
<p>

At this point, you can follow the steps presented in the next section to query 
the data in the store's <em>vehicleTable</em> or <em>rmvTable</em>, using 
either <em>Hive HQL</em> or <em>Big Data SQL</em>.

<p>
<a name="executing_hive_and_big_data_sql_queries"/>
<b><em>&mdash; Using Hive and Big Data SQL to Query Data in a KVStore &mdash;</em></b>
<p>

In order to query data in a KVStore from a <em>Big Data SQL</em> system (for example, 
a <em>Big Data Lite VM</em>), you must first create and map a Hive external table to 
the table defined in the store; whether the table is located in a 
<a href="#create_hive_external_vehicle_table_nonsecure"><b><code>non-secure store</code></b></a> 
or a <a href="#create_hive_external_vehicle_table_secure"><b><code>secure store</code></b></a>.
That is, given the configuration described in the previous section, login to the 
<em>Big Data Lite VM</em> and if the Oracle NoSQL Database <em>vehicleTable</em> 
and <em>rmvTable</em> are located in a 
<a href="#create_hive_external_vehicle_table_nonsecure"><b><code>non-secure store</code></b></a>, 
then enter the Hive CLI and type the following commands:

<pre>
  &gt; hive

  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: green;">vehicleTable</strong> (TYPE STRING, MAKE STRING, MODEL STRING, CLASS STRING, COLOR STRING, PRICE DOUBLE, COUNT INT) 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "vehicleTable",
                          <strong style="color: purple;">"oracle.kv.hadoop.hosts" = "bigdatalite.localdomain"</strong>);

  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: green;">rmvTable</strong> (ZIPCODE STRING, LASTNAME STRING, FIRSTNAME STRING, 
                    SSN BIGINT, 
                    GENDER STRING, 
                    LICENSE BINARY, 
                    PHONEINFO MAP&lt;STRING, STRING&gt;, 
                    ADDRESS STRUCT&lt;NUMBER:INT, STREET:STRING, UNIT:INT, CITY:STRING, STATE:STRING, ZIP:INT&gt;, 
                    VEHICLEINFO ARRAY&lt;STRUCT&lt;TYPE:STRING, MAKE:STRING, MODEL:STRING, CLASS:STRING, COLOR:STRING, VALUE:FLOAT, TAX:DOUBLE, PAID:BOOLEAN&gt;&gt;)
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "rmvTable",
                          <strong style="color: purple;">"oracle.kv.hadoop.hosts" = "bigdatalite.localdomain"</strong>);
</pre>

On the other hand, if the store in which <em>vehicleTable</em> and <em>rmvTable</em> are located is a 
<a href="#create_hive_external_vehicle_table_secure"><b><code>secure store</code></b></a>, 
then enter the Hive CLI and type commands like these:

<pre>
  &gt; hive

  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">vehicleTablePasswd</strong> (TYPE STRING, MAKE STRING, MODEL STRING, CLASS STRING, COLOR STRING, PRICE DOUBLE, COUNT INT) 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "vehicleTable",
                         <strong style="color: purple;">"oracle.kv.hadoop.hosts" = "bigdatalite.localdomain"</strong>,
                         <strong style="color: red;">"oracle.kv.security" = "/u01/nosql/example-user/security/hive-nosql.login", 
                         "oracle.kv.ssl.trustStore" = "/u01/nosql/example-user/security/client.trust", 
                         "oracle.kv.auth.username" = "example-user", 
                         "oracle.kv.auth.pwdfile.file" = "/u01/nosql/example-user/security/example-user.passwd"</strong>);

  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">vehicleTableWallet</strong> (TYPE STRING, MAKE STRING, MODEL STRING, CLASS STRING, COLOR STRING, PRICE DOUBLE, COUNT INT) 
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "vehicleTable",
                         <strong style="color: purple;">"oracle.kv.hadoop.hosts" = "bigdatalite.localdomain"</strong>,
                         <strong style="color: red;">"oracle.kv.security" = "/u01/nosql/example-user/security/hive-nosql.login", 
                         "oracle.kv.ssl.trustStore" = "/u01/nosql/example-user/security/client.trust", 
                         "oracle.kv.auth.username" = "example-user", 
                         "oracle.kv.auth.wallet.dir" = "/u01/nosql/example-user/security/example-user-wallet.dir"</strong>);

  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">rmvTablePasswd</strong> (ZIPCODE STRING, LASTNAME STRING, FIRSTNAME STRING, 
                    SSN BIGINT, 
                    GENDER STRING, 
                    LICENSE BINARY, 
                    PHONEINFO MAP&lt;STRING, STRING&gt;, 
                    ADDRESS STRUCT&lt;NUMBER:INT, STREET:STRING, UNIT:INT, CITY:STRING, STATE:STRING, ZIP:INT&gt;, 
                    VEHICLEINFO ARRAY&lt;STRUCT&lt;TYPE:STRING, MAKE:STRING, MODEL:STRING, CLASS:STRING, COLOR:STRING, VALUE:FLOAT, TAX:DOUBLE, PAID:BOOLEAN&gt;&gt;)
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "rmvTable",
                         <strong style="color: purple;">"oracle.kv.hadoop.hosts" = "bigdatalite.localdomain"</strong>,
                         <strong style="color: red;">"oracle.kv.security" = "/u01/nosql/example-user/security/hive-nosql.login", 
                         "oracle.kv.ssl.trustStore" = "/u01/nosql/example-user/security/client.trust", 
                         "oracle.kv.auth.username" = "example-user", 
                         "oracle.kv.auth.pwdfile.file" = "/u01/nosql/example-user/security/example-user.passwd"</strong>);

  hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS 
          <strong style="color: red;">rmvTableWallet</strong> (ZIPCODE STRING, LASTNAME STRING, FIRSTNAME STRING, 
                    SSN BIGINT, 
                    GENDER STRING, 
                    LICENSE BINARY, 
                    PHONEINFO MAP&lt;STRING, STRING&gt;, 
                    ADDRESS STRUCT&lt;NUMBER:INT, STREET:STRING, UNIT:INT, CITY:STRING, STATE:STRING, ZIP:INT&gt;, 
                    VEHICLEINFO ARRAY&lt;STRUCT&lt;TYPE:STRING, MAKE:STRING, MODEL:STRING, CLASS:STRING, COLOR:STRING, VALUE:FLOAT, TAX:DOUBLE, PAID:BOOLEAN&gt;&gt;)
          STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
          TBLPROPERTIES ("oracle.kv.kvstore" = "example-store",
                         "oracle.kv.hosts" = "kv-host-1:5000",
                         "oracle.kv.tableName" = "rmvTable",
                         <strong style="color: purple;">"oracle.kv.hadoop.hosts" = "bigdatalite.localdomain"</strong>,
                         <strong style="color: red;">"oracle.kv.security" = "/u01/nosql/example-user/security/hive-nosql.login", 
                         "oracle.kv.ssl.trustStore" = "/u01/nosql/example-user/security/client.trust", 
                         "oracle.kv.auth.username" = "example-user", 
                         "oracle.kv.auth.wallet.dir" = "/u01/nosql/example-user/security/example-user-wallet.dir"</strong>);
</pre>

At this point, you can execute <b><em>Hive queries</em></b> against the data in both the 
<a href="../../table/package-summary.html#execute_example_1_hive_query"><b><code>vehicleTable</code></b></a> 
and the <a href="#execute_example_2_hive_query"><b><code>rmvTable</code></b></a>. 
<p>

Note that although the tables in the KVStore are named <em>vehicleTable</em> and <em>rmvTable</em>, 
the Hive tables that are created above are named, respectively, <em>vehicleTable</em> and <em>rmvTable</em> 
if the KVStore is not configured for security, <em>vehicleTablePasswd</em> and <em>rmvTablePasswd</em> if the 
KVStore is secure and you store the password in a password file, and finally, <em>vehicleTableWallet</em> 
and <em>rmvTableWallet</em> if the KVStore is secure and you store the password in an Oracle Wallet. 
Also note that the value of the <code>oracle.kv.hadoop.hosts</code> property is 
<code>bigdatalite.localdomain</code>. This is the hostname of the 
<em>Big Data Lite VM</em>; thus, it is the name of the host on which the Hadoop 
DataNode(s) execute.
<p>

After you create and map the external Hive tables to their corresponding KVStore
tables, to then use <em>Big Data SQL</em> to query the data in those KVStore tables, 
you must apply the data model mapping 
<a href="#ondb_hive_ora_data_model_mapping_table"><b><code>presented above</code></b></a>, 
along with the schemas for the 
<a href="../../table/package-summary.html#schema_vehicle_table"><b><code>vehicleTable</code></b></a> 
and <a href="#appendix_a_table_schema_rmv_table"><b><code>rmvTable</code></b></a>, to 
create and map the corresponding Oracle RDBMS external tables to each of the Hive tables that you created. 
But before doing this, you must first create a user in the RDBMS, with the appropriate
privileges; as explained in the next section.

<p>
<a name="creating_rdbms_example_user"/>
<b><em>&mdash; Creating an Example User in the RDBMS &mdash;</em></b>
<p>

As of version <code>4.2</code>, the Big Data Lite VM now employs the 
<em>multitenant</em> feature of the Oracle RDBMS. As a result, <em>Big Data SQL</em>
commands and queries must be executed by a user with the appropriate privileges. Thus,
before creating any tables or executing any <em>Big Data SQL</em> queries,
you should first create an example user by executing commands like the following:

<pre>
  &gt; sqlplus sys/welcome1@orcl as sysdba

  SQL&gt; CREATE TABLESPACE NOSQL_EXAMPLE_TBLSPACE
         DATAFILE '/u01/app/oracle/oradata/cdb/orcl/nosql_example_tblspace.dbf'
           SIZE 500M AUTOEXTEND ON MAXSIZE UNLIMITED
         NOLOGGING
         DEFAULT COMPRESS
         EXTENT MANAGEMENT
           LOCAL AUTOALLOCATE
         SEGMENT SPACE MANAGEMENT
           AUTO;

  SQL&gt; CREATE USER NOSQL_EXAMPLE_USER
         IDENTIFIED BY welcome1
         DEFAULT TABLESPACE NOSQL_EXAMPLE_TBLSPACE
         QUOTA UNLIMITED ON NOSQL_EXAMPLE_TBLSPACE;

  SQL&gt; GRANT CREATE ANY DIRECTORY TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT DROP ANY DIRECTORY TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT CREATE TABLE TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT CREATE VIEW TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT CREATE MINING MODEL TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT CREATE SESSION TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT CREATE PROCEDURE TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT ADVISOR TO NOSQL_EXAMPLE_USER;
  SQL&gt; GRANT EXECUTE,READ,WRITE ON DIRECTORY
         DEFAULT_DIR TO NOSQL_EXAMPLE_USER
         WITH GRANT OPTION;
  SQL&gt; GRANT EXECUTE,READ,WRITE ON DIRECTORY
         ORACLE_BIGDATA_CONFIG TO NOSQL_EXAMPLE_USER
         WITH GRANT OPTION;

  SQL&gt; exit
</pre>

At this point, you can login as the new user and execute the <em>Big Data SQL</em>
commands and queries presented in the next section to query the data in the 
associated KVStore tables.

<p>
<a name="executing_big_data_sql_queries"/>
<b><em>&mdash; Using Big Data SQL to Query Data in a KVStore &mdash;</em></b>
<p>

As described in the previous sections, to use <em>Big Data SQL</em> to query the 
data in the KVStore, you must create and map Oracle RDBMS external tables to each Hive 
table you created for the corresponding KVStore table. To do this, login to the 
Oracle RDBMS as the new user and execute the commands and queries shown below at the 
<code>sqlplus</code> command prompt:

<pre>
  &gt; sqlplus NOSQL_EXAMPLE_USER/welcome1@orcl

  SQL&gt; CREATE TABLE <strong style="color: red;">vehicleTable</strong> (type VARCHAR2(10), make VARCHAR2(12), model VARCHAR2(20),
                                      class VARCHAR2(40), color VARCHAR2(20), price NUMBER(8,2),
                                      count NUMBER)
                        ORGANIZATION EXTERNAL (TYPE ORACLE_HIVE DEFAULT DIRECTORY DEFAULT_DIR
                                               ACCESS PARAMETERS (com.oracle.bigdata.debug=true
                                               com.oracle.bigdata.log.opt=normal)) REJECT LIMIT UNLIMITED;

  SQL&gt; CREATE TABLE <strong style="color: red;">vehicleTablePasswd</strong> (type VARCHAR2(10), make VARCHAR2(12), model VARCHAR2(20),
                                            class VARCHAR2(40), color VARCHAR2(20), price NUMBER(8,2),
                                            count NUMBER) 
                        ORGANIZATION EXTERNAL (TYPE ORACLE_HIVE DEFAULT DIRECTORY DEFAULT_DIR
                                               ACCESS PARAMETERS (com.oracle.bigdata.debug=true
                                               com.oracle.bigdata.log.opt=normal)) REJECT LIMIT UNLIMITED;

  SQL&gt; CREATE TABLE <strong style="color: red;">vehicleTableWallet</strong> (type VARCHAR2(10), make VARCHAR2(12), model VARCHAR2(20), 
                                            class VARCHAR2(40), color VARCHAR2(20), price NUMBER(8,2), 
                                            count NUMBER) 
                        ORGANIZATION EXTERNAL (TYPE ORACLE_HIVE DEFAULT DIRECTORY DEFAULT_DIR
                                               ACCESS PARAMETERS (com.oracle.bigdata.debug=true
                                               com.oracle.bigdata.log.opt=normal)) REJECT LIMIT UNLIMITED;

  SQL&gt; CREATE TABLE <strong style="color: red;">rmvTable</strong> (ZIPCODE VARCHAR2(7), LASTNAME VARCHAR2(20), FIRSTNAME VARCHAR2(20),
                                            SSN NUMBER, GENDER VARCHAR2(6), LICENSE VARCHAR2(9), PHONEINFO VARCHAR2(67), ADDRESS VARCHAR2(100),
                                            VEHICLEINFO VARCHAR2(1000))
                        ORGANIZATION EXTERNAL (TYPE ORACLE_HIVE DEFAULT DIRECTORY DEFAULT_DIR
                                               ACCESS PARAMETERS (com.oracle.bigdata.debug=true
                                               com.oracle.bigdata.log.opt=normal)) REJECT LIMIT UNLIMITED;

  SQL&gt; CREATE TABLE <strong style="color: red;">rmvTablePasswd</strong> (ZIPCODE VARCHAR2(7), LASTNAME VARCHAR2(20), FIRSTNAME VARCHAR2(20),
                                            SSN NUMBER, GENDER VARCHAR2(6), LICENSE VARCHAR2(9), PHONEINFO VARCHAR2(67), ADDRESS VARCHAR2(100),
                                            VEHICLEINFO VARCHAR2(1000))
                        ORGANIZATION EXTERNAL (TYPE ORACLE_HIVE DEFAULT DIRECTORY DEFAULT_DIR
                                               ACCESS PARAMETERS (com.oracle.bigdata.debug=true
                                               com.oracle.bigdata.log.opt=normal)) REJECT LIMIT UNLIMITED;

  SQL&gt; CREATE TABLE <strong style="color: red;">rmvTableWallet</strong> (ZIPCODE VARCHAR2(7), LASTNAME VARCHAR2(20), FIRSTNAME VARCHAR2(20),
                                            SSN NUMBER, GENDER VARCHAR2(6), LICENSE VARCHAR2(9), PHONEINFO VARCHAR2(67), ADDRESS VARCHAR2(100),
                                            VEHICLEINFO VARCHAR2(1000))
                        ORGANIZATION EXTERNAL (TYPE ORACLE_HIVE DEFAULT DIRECTORY DEFAULT_DIR
                                               ACCESS PARAMETERS (com.oracle.bigdata.debug=true
                                               com.oracle.bigdata.log.opt=normal)) REJECT LIMIT UNLIMITED;
</pre>

Note that, except for the name specified for the table, the commands related to 
<em>vehicleTable</em> are identical. Similarly, the commands related to 
<em>rmvTable</em> are also identical except for the table name. This 
is how an Oracle external table is mapped to the desired Hive table. If you want 
to create an Oracle external table with a name that is different than the Hive table 
to which it is mapped, then you can specify the <code>com.oracle.bigdata.tablename</code> 
property in the <code>ACCESS PARAMETERS</code>; for example, 

<pre>
  SQL&gt; CREATE TABLE <strong style="color: red;">bigdataSQL_vehicleTableWallet</strong> (type VARCHAR2(10), make VARCHAR2(12), model VARCHAR2(20), 
                                            class VARCHAR2(40), color VARCHAR2(20), price NUMBER(8,2), 
                                            count NUMBER) 
                        ORGANIZATION EXTERNAL (TYPE ORACLE_HIVE DEFAULT DIRECTORY DEFAULT_DIR
                                               ACCESS PARAMETERS (<strong style="color: red;">com.oracle.bigdata.tablename=vehicleTableWallet</strong> 
                                               com.oracle.bigdata.debug=true
                                               com.oracle.bigdata.log.opt=normal)) REJECT LIMIT UNLIMITED;
</pre>

Note that if you do not employ the <code>com.oracle.bigdata.tablename</code> 
property, then the name you specify for the Oracle external table 
<b><em>must be indentical to</em></b> the name of the corresponding 
Hive external table. Specifically, if you specify a name for the Oracle external
table different than the name of the corresponding Hive table, and you do 
not employ the <code>com.oracle.bigdata.tablename</code> property to 
specify the name of the desired Hive table, then an error will occur.
<p>

After creating the Oracle external tables above, you can execute SQL queries 
against the corresponding KVStore table data. For example, to query the data 
contained in the KVStore's <em>vehicleTable</em>, you can execute queries 
such as:

<pre>
  SQL&gt; set linesize 200;

  SQL&gt; select * from vehicleTable;
  SQL&gt; select count(*) from vehicleTable;
  SQL&gt; select min(price) from vehicleTable;
  SQL&gt; select * from vehicleTable where make='GM';
  SQL&gt; select * from vehicleTable where model='Equinox';
  SQL&gt; select * from vehicleTable where model='Camaro';
  SQL&gt; select * from vehicleTable where model='Silverado1500';

  SQL&gt; select * from vehicleTablePasswd;
  SQL&gt; select count(*) from vehicleTablePasswd;
  SQL&gt; select min(price) from vehicleTablePasswd;
  SQL&gt; select * from vehicleTablePasswd where make='GM';
  SQL&gt; select * from vehicleTablePasswd where model='Equinox';
  SQL&gt; select * from vehicleTablePasswd where model='Camaro';
  SQL&gt; select * from vehicleTablePasswd where model='Silverado1500';

  SQL&gt; select * from vehicleTableWallet;
  SQL&gt; select min(price) from vehicleTableWallet;
  SQL&gt; select * from vehicleTableWallet where make='GM';
  SQL&gt; select * from vehicleTableWallet where model='Equinox';
  SQL&gt; select * from vehicleTableWallet where model='Camaro';
  SQL&gt; select * from vehicleTableWallet where model='Silverado1500';
</pre>

Similarly, to query the data contained in the KVStore's <em>rmvTable</em>, 
you can execute simple queries such as these:

<pre>
  SQL&gt; set linesize 200;

  SQL&gt; select * from rmvTable;
  SQL&gt; select * from rmvTablePasswd;
  SQL&gt; select * from rmvTableWallet;

  SQL&gt; select lastname,firstname,gender,address from rmvTable; 
  SQL&gt; select lastname,firstname,gender,address from rmvTablePasswd; 
  SQL&gt; select lastname,firstname,gender,address from rmvTableWallet; 

  SQL&gt; select min(ssn) from rmvTable;
  SQL&gt; select min(ssn) from rmvTablePasswd;
  SQL&gt; select min(ssn) from rmvTableWallet;

  SQL&gt; select count(*) from rmvTable;
  SQL&gt; select count(*) from rmvTablePasswd;
  SQL&gt; select count(*) from rmvTableWallet;

  SQL&gt; select firstname,lastname,phoneinfo from rmvTable; 
  SQL&gt; select firstname,lastname,phoneinfo from rmvTablePasswd; 
  SQL&gt; select firstname,lastname,phoneinfo from rmvTableWallet; 

  SQL&gt; select vehicleinfo from rmvTable;
  SQL&gt; select vehicleinfo from rmvTablePasswd;
  SQL&gt; select vehicleinfo from rmvTableWallet;
</pre>

To achieve the more complicated query functionality demonstrated by the 
correspondng Hive queries that dereferenced individual field values of the 
non-primitive data types, you can employ either
<a href="http://docs.oracle.com/cd/B19306_01/appdev.102/b14251/adfns_regexp.htm"><b><code>Oracle Regular Expression Functions</code></b></a>
such as <code>REGEXP_LIKE</code> and <code>REGEXP_SUBSTR</code>, or 
<a href="https://blogs.oracle.com/jsondb/entry/s"><b><code>Oracle SQL JSON Operators</code></b></a>
such as <code>JSON_QUERY</code> and <code>JSON_EXISTS</code> (or a combination). 
For example, consider the following queries that use 
<a href="http://docs.oracle.com/cd/B19306_01/appdev.102/b14251/adfns_regexp.htm"><b><code>Oracle Regular Expressions</code></b></a>:

<pre>
  SQL&gt; select firstname,lastname,address,vehicleinfo from rmvTable where REGEXP_LIKE (lastname, '^[H].*');
  SQL&gt; select firstname,lastname,address,vehicleinfo from rmvTablePasswd where REGEXP_LIKE (lastname, '^[H].*');
  SQL&gt; select firstname,lastname,address,vehicleinfo from rmvTableWallet where REGEXP_LIKE (lastname, '^[H].*');

  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary make: GM" 
       from rmvTable where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"make":"GM"');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary make: GM" 
       from rmvTablePasswd where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"make":"GM"');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary make: GM" 
       from rmvTableWallet where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"make":"GM"');

  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Equinox" 
       from rmvTable where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Equinox"');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Equinox" 
       from rmvTablePasswd where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Equinox"');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Equinox" 
       from rmvTableWallet where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Equinox"');

  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Camaro" 
       from rmvTable where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Camaro"');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Camaro" 
       from rmvTablePasswd where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Camaro"');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Camaro" 
       from rmvTableWallet where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Camaro"');

  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Silverado" 
       from rmvTable where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Silverado');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Silverado" 
       from rmvTablePasswd where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Silverado');
  SQL&gt; select firstname,lastname,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary model: Silverado" 
       from rmvTableWallet where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"model":"Silverado');

  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary fee NOT paid" 
       from rmvTable where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"paid":false');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary fee NOT paid" 
       from rmvTablePasswd where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"paid":false');
  SQL&gt; select firstname,lastname,address,REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1) "Primary fee NOT paid" 
       from rmvTableWallet where REGEXP_LIKE (REGEXP_SUBSTR(VEHICLEINFO, '\{([[:alnum:]":,\.]+){1,3}\}',1,1), '"paid":false');
</pre>

Finally, consider the following example queries that employ 
<a href="https://blogs.oracle.com/jsondb/entry/s"><b><code>Oracle SQL JSON Operators</code></b></a>:

<pre>
  SQL&gt; select FIRSTNAME, LASTNAME, j.ADDRESS.street, j.ADDRESS.city, j.ADDRESS.state, j.VEHICLEINFO.model from rmvTable j;
  SQL&gt; select FIRSTNAME, LASTNAME, j.ADDRESS.street, j.ADDRESS.city, j.ADDRESS.state, j.VEHICLEINFO.model from rmvTablePasswd j;
  SQL&gt; select FIRSTNAME, LASTNAME, j.ADDRESS.street, j.ADDRESS.city, j.ADDRESS.state, j.VEHICLEINFO.model from rmvTableWallet j;

  SQL&gt; select JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) from rmvTable;
  SQL&gt; select JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) from rmvTablePasswd;
  SQL&gt; select JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) from rmvTableWallet;

  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: GM" 
       from rmvTable WHERE JSON_QUERY(VEHICLEINFO, '$[0].make' WITH CONDITIONAL WRAPPER) LIKE '%GM%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: GM" 
       from rmvTablePasswd WHERE JSON_QUERY(VEHICLEINFO, '$[0].make' WITH CONDITIONAL WRAPPER) LIKE '%GM%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: GM" 
       from rmvTableWallet WHERE JSON_QUERY(VEHICLEINFO, '$[0].make' WITH CONDITIONAL WRAPPER) LIKE '%GM%';

  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Equinox"
       from rmvTable WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Equinox%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Equinox"
       from rmvTablePasswd WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Equinox%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Equinox"
       from rmvTableWallet WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Equinox%';

  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Camaro"
       from rmvTable WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Camaro%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Camaro"
       from rmvTablePasswd WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Camaro%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Camaro"
       from rmvTableWallet WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Camaro%';

  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Silverado"
       from rmvTable WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Silverado%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Silverado"
       from rmvTablePasswd WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Silverado%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(VEHICLEINFO, '$[0]' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle: Silverado"
       from rmvTableWallet WHERE JSON_QUERY(VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) LIKE '%Silverado%';

  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(
         VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle Model",
           JSON_QUERY(VEHICLEINFO, '$[0].value' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle Value",
           JSON_QUERY(VEHICLEINFO, '$[0].tax' WITH CONDITIONAL WRAPPER) AS "Tax Owed",
           JSON_QUERY(VEHICLEINFO, '$[0].paid' WITH CONDITIONAL WRAPPER) AS "Tax Paid" 
       from rmvTable WHERE JSON_QUERY(VEHICLEINFO, '$[0].make' WITH CONDITIONAL WRAPPER) LIKE '%GM%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(
         VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle Model",
           JSON_QUERY(VEHICLEINFO, '$[0].value' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle Value",
           JSON_QUERY(VEHICLEINFO, '$[0].tax' WITH CONDITIONAL WRAPPER) AS "Tax Owed",
           JSON_QUERY(VEHICLEINFO, '$[0].paid' WITH CONDITIONAL WRAPPER) AS "Tax Paid" 
       from rmvTablePasswd WHERE JSON_QUERY(VEHICLEINFO, '$[0].make' WITH CONDITIONAL WRAPPER) LIKE '%GM%';
  SQL&gt; select FIRSTNAME,LASTNAME,ADDRESS,JSON_QUERY(
         VEHICLEINFO, '$[0].model' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle Model",
           JSON_QUERY(VEHICLEINFO, '$[0].value' WITH CONDITIONAL WRAPPER) AS "Primary Vehicle Value",
           JSON_QUERY(VEHICLEINFO, '$[0].tax' WITH CONDITIONAL WRAPPER) AS "Tax Owed",
           JSON_QUERY(VEHICLEINFO, '$[0].paid' WITH CONDITIONAL WRAPPER) AS "Tax Paid" 
       from rmvTableWallet WHERE JSON_QUERY(VEHICLEINFO, '$[0].make' WITH CONDITIONAL WRAPPER) LIKE '%GM%';

  SQL&gt; select JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) AS "Street"
       from rmvTable WHERE JSON_EXISTS(ADDRESS, '$.street');
  SQL&gt; select JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) AS "Street"
       from rmvTablePasswd WHERE JSON_EXISTS(ADDRESS, '$.street');
  SQL&gt; select JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) AS "Street"
       from rmvTableWallet WHERE JSON_EXISTS(ADDRESS, '$.street');

  SQL&gt; select JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) AS "Street"
       from rmvTable WHERE JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) LIKE '%High Street%';
  SQL&gt; select JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) AS "Street"
       from rmvTablePasswd WHERE JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) LIKE '%High Street%';
  SQL&gt; select JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) AS "Street"
       from rmvTableWallet WHERE JSON_QUERY(ADDRESS, '$.street' WITH CONDITIONAL WRAPPER) LIKE '%High Street%';
</pre>

<a name="appendix_a_create_populate_rmv_table"/>
<h3>Appendix A: Create and Populate 'rmvTable' with Example Data</h4> 

In order to run any of the example Hive or Big Data SQL queries presented in this 
document, a KVStore &mdash; either secure or non-secure &mdash; must first be 
deployed, and a table must be created and populated with data. Thus, before 
attempting to execute any queries, either deploy a non-secure KVStore using the 
steps outlined in 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>Appendix A</code></b></a> 
of the <em>Hadoop/Table API Example</em>, or start a KVStore configured for security 
using the steps presented in 
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>Appendix B</code></b></a> 
of that document. Once a KVStore has been deployed, the standalone Java program 
<code>LoadRmvTable</code> can then be run against either type of store to create 
a table with the name and schema required by the various example queries, and 
then populate the table with rows of data consistent with that schema. Once 
the table is created and populated with example data, queries such as those 
presented in <a href="#hive_query_example_1"><b><code>Example 1</code></b></a>, 
<a href="#hive_query_example_2"><b><code>Example 2</code></b></a>, or the 
section on <a href="#oracle_bigdata_sql"><b><code>Big Data SQL</code></b></a>
can be executed against the data in that table.
<p>

<a name="appendix_a_schema_rmv_table"/>
<h4>Schema for the Example 'rmvTable'</h4> 

To execute any of the example queries presented in 
<a href="#hive_query_example_2"><b><code>Example 2</code></b></a>, 
of this document a table named <em>rmvTable</em> &mdash; having the schema 
shown in the table below &mdash; must be created in the KVStore that was 
deployed for that example; where the data types specified in the schema are 
defined by the Oracle NoSQL Database Table API (see 
<a href="../../../../javadoc/oracle/kv/table/FieldDef.Type.html"><b><code>oracle.kv.table.FieldDef.Type</code></b></a>).
<p>

<a name="appendix_a_table_schema_rmv_table"/>
<table border="1" style="width:75%;border-collapse:collapse">
  <caption>'rmvTable' Schema</caption>
  <tr>
    <th>Field Name</th>
    <th>Field Type</th>
  </tr>
  <tr>
    <td align="center">zipcode</td>
    <td align="center">STRING</td>
  </tr>
  <tr>
    <td align="center">lastname</td>
    <td align="center">STRING</td>
  </tr>
  <tr>
    <td align="center">firstname</td>
    <td align="center">STRING</td>
  </tr>
  <tr>
    <td align="center">ssn</td>
    <td align="center">LONG</td>
  </tr>
  <tr>
    <td align="center">gender</td>
    <td align="center">ENUM</td>
  </tr>
  <tr>
    <td align="center">license</td>
    <td align="center">FIXED_BINARY(9)</td>
  </tr>
  <tr>
    <td align="center">phoneinfo</td>
    <td align="center">MAP(STRING)</td>
  </tr>

  <tr>
    <td align="center">address</td>
    <td align="center">RECORD(field_name : field_type)</td>
  </tr>
  <tr>
    <th rowspan="6">address Record Schema</th>
    <td align="center">number : INTEGER</td>
  </tr>
  <tr>
    <td align="center">street : STRING</td>
  </tr>
  <tr>
    <td align="center">unit : INTEGER</td>
  </tr>
  <tr>
    <td align="center">city : STRING</td>
  </tr>
  <tr>
    <td align="center">state : STRING</td>
  </tr>
  <tr>
    <td align="center">zip : INTEGER</td>
  </tr>
  <tr>
    <td align="center">vehicleinfo</td>
    <td align="center">ARRAY(RECORD(field_name : field_type))</td>
  </tr>
  <tr>
    <th rowspan="8">vehicleinfo Element Record Schema</th>
    <td align="center">type : STRING</td>
  </tr>
  <tr>
    <td align="center">make : STRING</td>
  </tr>
  <tr>
    <td align="center">model : STRING</td>
  </tr>
  <tr>
    <td align="center">class : STRING</td>
  </tr>
  <tr>
    <td align="center">color : STRING</td>
  </tr>
  <tr>
    <td align="center">value : FLOAT</td>
  </tr>
  <tr>
    <td align="center">tax : DOUBLE</td>
  </tr>
  <tr>
    <td align="center">paid : BOOLEAN</td>
  </tr>
  <tr>
    <th colspan="5">Primary Key Field Names</th>
  </tr>
  <tr>
    <td align="center">zipcode</td>
    <td align="center">lastname</td>
    <td align="center">firstname</td>
    <td align="center">ssn</td>
  </tr>
  <tr>
    <th colspan="3">Shard Key Field Names</th>
  </tr>
  <tr>
    <td align="center">zipcode</td>
  </tr>
</table>
<p>

Thus, <em>rmvTable</em> will consist of rows of data the <em>Registry of Motor Vehicles</em>
might maintain about vehicle owners who have registered a primary vehicle and (optionally) a 
second and maybe a third vehicle. In addition to personal information about each 
owner &mdash; such as name, address, gender, phone number(s), etc. &mdash; each row of 
data also contains an array in which each element of the array is a record whose contents 
consists of information about each vehicle the owner registers. For example, in addition to 
vehicle attributes such as the make, model, color, etc., the record will also contain the 
vehicle's assessed value, registration fee (the <em>tax</em>), and whether or not the 
owner has paid the fee. Although the table schema presented above may seem a bit contrived, 
it is intended to demonstrate a broad spectrum of data types from the 
Oracle NoSQL Database Table API.
<p>

Although you can enter individual commands in the admin CLI to create a table with the above 
schema, an alternative mechanism is to follow the instructions presented in the next sections 
to compile and execute the <code>LoadRmvTable</code> program; which will use the 
<a href="http://docs.oracle.com/cd/NOSQL/html/GettingStartedGuideTables/ddl_overview.html"><b><code>Oracle NoSQL Database Data Definition Language (<em>DDL</em>)</code></b></a> to <em>create</em> as well as 
<em>populate</em> the desired table.

<p>
<a name="appendix_a_compile_load_rmv_table"/>
<b><em>&mdash; Compiling the LoadRmvTable Program &mdash;</em></b>
<p>

After the KVStore 
&mdash; either 
<a href="../../table/package-summary.html#appendix_a_nonsecure_ondb_store"><b><code>non-secure</code></b></a> 
or 
<a href="../../table/package-summary.html#appendix_b_secure_ondb_store"><b><code>secure</code></b></a> 
&mdash; 
has been deployed, before executing <code>LoadRmvTable</code>, that program must first 
be compiled. To do this, type the following command from the OS command line:

<pre>
  &gt; cd /opt/ondb/kv
  &gt; javac -classpath lib/kvstore.jar:examples examples/hadoop/hive/table/LoadRmvTable.java
</pre>

which should produce the file: 

<pre>
  /opt/ondb/kv/examples/hadoop/hive/table/LoadRmvTable.class
</pre>

<p>
<a name="appendix_a_create_populate_rmv_table_non_secure"/>
<b><em>&mdash; Creating and Populating 'rmvTable' with Example Data in a <strong style="color: green;">Non-Secure</strong> KVStore &mdash;</em></b>
<p>

To execute <code>LoadRmvTable</code> to create and then populate the 
table named <em>rmvTable</em> with example data in a KVStore configured 
for <em>non-secure</em> access, type the following at the command line of a 
node that has network connectivity with a node running the admin service 
(for example, <em>kv-host-1</em> itself):

<pre>
  &gt; cd /opt/ondb/kv
  &gt; java -classpath lib/kvstore.jar:examples hadoop.table.LoadRmvTable \
             -store example-store -host kv-host-1 -port 5000 -nops 79 [-delete]
</pre>

where the parameters <code>-store</code>, <code>-host</code>, <code>-port</code>,
and <code>-nops</code> are required. 
<p>

In the example command line above, the argument <code>-nops 79</code> specifies 
that 79 rows be written to the <em>rmvTable</em>. If more or less 
than that number of rows is desired, then the value of the <code>-nops</code> 
parameter should be changed.
<p>

If <code>LoadRmvTable</code> is executed a second time and the 
optional <code>-delete</code> parameter is specified, then all rows added by
any previous executions of <code>LoadRmvTable</code> are deleted from the
table prior to adding the new rows. Otherwise, all pre-existing rows are left
in place, and the number of rows in the table will be increased by the specified
<code>-nops</code> number of new rows.
<p>

<a name="appendix_a_create_populate_rmv_table_secure"/>
<b><em>&mdash; Creating and Populating 'rmvTable' with Example Data in a <strong style="color: red;">Secure</strong> KVStore &mdash;</em></b>
<p>

To execute <code>LoadRmvTable</code> against a <em>secure</em> KVStore deployed 
and provisioned with a non-administrative user employing the steps presented in 
<a href="../../table/package-summary.html#appendix_b_step_4_generate_non_admin_user"><b><code>Appendix B</code></b></a> 
of the <em>Hadoop/Table API Example</em>, an additonal parameter must be added to 
the command line above. That is, type the following:

<pre>
  &gt; cd /opt/ondb/kv
  &gt; javac -classpath lib/kvclient.jar:LoadRmvTable examples/hadoop/hive/table/LoadRmvTable.java
  &gt; <strong style="color: purple;">cp /opt/ondb/example-store/security/client.trust /tmp</strong>
  &gt; java -classpath lib/kvstore.jar:examples hadoop.table.LoadRmvTable \
             -store example-store -host kv-host-1 -port 5000 -nops 79 \
             <strong style="color: red;">-security /tmp/example-user-client-pwdfile.login</strong>
             [-delete]
</pre>

where the role of the single additonal <code>-security</code> parameter is identical 
to the role of that parameter when executing the <code>LoadVehicleTable</code> program; 
as explained in 
<a href="../../table/package-summary.html#single_login_properties_security_parameter_explanation"><b><code>Appendix B</code></b></a> 
of the <em>Hadoop/Table API Example</em>.
<p>

At this point, the <em>rmvTable</em> created in the specified KVStore (non-secure or secure) 
should be populated with the desired example data; which can then be queried via Hive HQL or 
Big Data SQL.

<a name="appendix_b_set_hive_aux_jars_path_nonsecure"/>
<h3>Appendix B: Setting the HIVE_AUX_JARS_PATH Envrionment Variable</h3>

In order to make the 
<a href="#ondb_hive_integration_classes"><b><code>Table API Hive Integration classes</code></b></a> 
available to the Hive client's Java VM, as well as the Java VMs of the 
DataNodes of the Hadoop Cluster, <code>kvclient.jar</code> must be added 
to the classpaths of those respective VMs. To do this, <code>kvclient.jar</code> 
must be added to the Hive client's <code>HIVE_AUX_JARS_PATH</code> environment 
variable. One way to achieve this is to simply export a new value for 
<code>HIVE_AUX_JARS_PATH</code> from the Hive client's command line; that is,

<pre>
  &gt; export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH,/opt/ondb/kv/lib/kvclient.jar
</pre>

The other, more permanent (and <em>preferable</em>), way to achieve this is to modify 
the <code>hive-env.sh</code> script located in the <code>HIVE_CONF_DIR</code>
of the Hive client. For example,

<pre>
  &gt; cd $HIVE_CONF_DIR
  &gt; edit hive-env.sh

    if [ -z "$HIVE_AUX_JARS_PATH" ]; then
        export HIVE_AUX_JARS_PATH=/opt/ondb/kv/lib/kvclient.jar
    else
        export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH,/opt/ondb/kv/lib/kvclient.jar
    fi
</pre>

It is important to note that in both cases above, the separator that is used 
is a <b><em>comma</em></b>, not a colon.

<a name="appendix_c_hive_ondb_security"/>
<h3>Appendix C: Hive and Oracle NoSQL Security</h3>

With respect to running Hive queries against table data contained in a secure KVStore, 
a particularly important issue to address involves the configuration of the Hive client 
environment, as well as the creation and installation of a set of artifacts; which 
together support the communication of user credentials to the various components that 
participate in the execution of a query. Because the execution of a Hive query can  
result in the initiation of a MapReduce job, before proceeding you should 
familiarize yourself with the build model presented in 
<a href="../../table/package-summary.html#appendix_c_secure_kvclient_packaging_model"><b><code>Appendix C</code></b></a> 
of the <em>Hadoop/Table API Example</em>; which describes the necessary artifacts in 
detail &mdash; their purpose, as well as how to generate and install them.

<p>
<a name="appendix_c_generate_security_artifacts"/>
<b><em>&mdash; Generating the Login, Trust, and Password Artifacts &mdash;</em></b>
<p>

To execute a Hive query against a secure KVStore, the necessary public and private 
credentials must be incorporated in the definition of the Hive table that will be 
queried. To do this, in a fashion similar to that presented in 
<a href="../../table/package-summary.html#appendix_c_secure_kvclient_packaging_model"><b><code>Appendix C</code></b></a> 
of the <em>Hadoop/Table API Example</em>, you must create artifacts like those shown 
below, and store them on the Hive client's local file system; for example,  

<pre>
  /tmp
    client.trust
    hive-nosql.login
    example-user.passwd
</pre>

Or, if wallet storage will be employed:

<pre>
  /tmp
    client.trust
    hive-nosql.login
    /example-user-wallet.dir
      cwallet.sso
</pre>

where <code>hive-nosql.login</code> is analogous (actually, identical) 
to <code>example-user-server.login</code> in 
<a href="../../table/package-summary.html#appendix_c_secure_kvclient_packaging_model"><b><code>Appendix C</code></b></a>. 

<p>
<a name="appendix_c_generate_server_side_jar"/>
<b><em>&mdash; Generating the Server Side JAR File &mdash;</em></b>
<p>

After creating the files above, the following <em>server side</em> JAR file 
should be generated so that it can be added to the <code>HIVE_AUX_JARS_PATH</code> 
environment variable (see below). That is, type the following at the command line: 

<pre>
  &gt; cd /tmp
  &gt; jar cvf /opt/ondb/kv/examples/hive-nosql-server.jar client.trust
  &gt; jar uvf /opt/ondb/kv/examples/hive-nosql-server.jar hive-nosql.login
</pre>

which produces the JAR file named <code>hive-nosql-server.jar</code>, with contents 
that include only <em>public</em> credentials and which look something like:

<pre>
     0 Mon May 04 13:01:04 PDT 2015 META-INF/
    68 Mon May 04 13:01:04 PDT 2015 META-INF/MANIFEST.MF
   508 Wed Apr 22 12:23:32 PDT 2015 client.trust
   255 Mon May 04 11:30:54 PDT 2015 hive-nosql.login
</pre>

<p>
<a name="appendix_c_set_hive_aux_jars_path_secure"/>
<b><em>&mdash; Setting the <code>HIVE_AUX_JARS_PATH</code> Environment Variable &mdash;</em></b>
<p>

Recall that <a href="#appendix_b_set_hive_aux_jars_path_nonsecure"><b><code>Appendix B</code></b></a> 
of this document described how to add <code>kvclient.jar</code> to the Hive client's 
<code>HIVE_AUX_JARS_PATH</code> environment variable in order to run a Hive query 
against a <em>non-secure</em> KVStore. In order to run a Hive query against a 
<em>secure</em> KVStore, in addition to adding <code>kvclient.jar</code>, a number 
of other JAR files must also be added to <code>HIVE_AUX_JARS_PATH</code>; depending 
on how the password is stored.
<p>

If the password is stored in a password file, then in addition to <code>kvclient.jar</code>, 
the <code>hive-nosql-server.jar</code> file must also be added to <code>HIVE_AUX_JARS_PATH</code>;
that is, do one of the following:

<pre>
  &gt; export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH,/opt/ondb/kv/lib/kvclient.jar,/opt/ondb/kv/examples/hive-nosql-server.jar
</pre>

or

<pre>
  &gt; cd HIVE_CONF_DIR
  &gt; edit hive-env.sh

    if [ -z "$HIVE_AUX_JARS_PATH" ]; then
        export HIVE_AUX_JARS_PATH=/opt/ondb/kv/lib/kvclient.jar,/opt/ondb/kv/examples/hive-nosql-server.jar
    else
        export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH,/opt/ondb/kv/lib/kvclient.jar,/opt/ondb/kv/examples/hive-nosql-server.jar
    fi
</pre>

On the other hand, if the password is stored in an Oracle Wallet, then you would 
also add three wallet related JAR files that are only provided with the Enterprise 
Edition of Oracle NoSQL Database. That is, you would do one of the following: 

<pre>
  &gt; export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH, \
                                  /opt/ondb/kv/lib/kvclient.jar,  \
                                  <strong style="color: red;">/opt/ondb/kv/lib/oraclepki.jar, \
                                  /opt/ondb/kv/lib/osdt_cert.jar, \
                                  /opt/ondb/kv/lib/osdt_core.jar, \</strong>
                                  /opt/ondb/kv/examples/hive-nosql-server.jar
</pre>
or
<pre>
  &gt; cd HIVE_CONF_DIR
  &gt; edit hive-env.sh

    if [ -z "$HIVE_AUX_JARS_PATH" ]; then
        export HIVE_AUX_JARS_PATH=/opt/ondb/kv/lib/kvclient.jar,  \
                                  <strong style="color: red;">/opt/ondb/kv/lib/oraclepki.jar, \
                                  /opt/ondb/kv/lib/osdt_cert.jar, \
                                  /opt/ondb/kv/lib/osdt_core.jar, \</strong>
                                  /opt/ondb/kv/examples/hive-nosql-server.jar
    else
        export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH, \
                                  /opt/ondb/kv/lib/kvclient.jar,  \
                                  <strong style="color: red;">/opt/ondb/kv/lib/oraclepki.jar, \
                                  /opt/ondb/kv/lib/osdt_cert.jar, \
                                  /opt/ondb/kv/lib/osdt_core.jar, \</strong>
                                  /opt/ondb/kv/examples/hive-nosql-server.jar
    fi
</pre>

Once Hive has been configured for Oracle NoSQL Database security in the manner 
just presented, you can then incorporate the necessary artifacts in your Hive 
external table definition in the fashion presented previously in this document; 
for <em>vehicleTable</em> in 
<a href="#create_hive_external_vehicle_table_secure"><b><code>Example 1</code></b></a>, 
and for <em>rmvTable</em> in 
<a href="#create_hive_external_rmv_table_secure"><b><code>Example 2</code></b></a>.
Note that specifying the security related artifacts in the manner presented 
in those examples is akin to specifying that information on the command line 
(rather than via classpath resources) when initiating a MapReduce job.</div>
</div>
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../overview-summary.html">Overview</a></li>
<li class="navBarCell1Rev">Package</li>
<li>Class</li>
<li><a href="package-use.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html">Help</a></li>
</ul>
<div class="aboutLanguage"><em><b>Oracle NoSQL Database Examples</b><br><font size=\"-1\"> version 12cR1.3.5.2</font>
      </em></div>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../hadoop/package-summary.html">Prev Package</a></li>
<li><a href="../../../hadoop/table/package-summary.html">Next Package</a></li>
</ul>
<ul class="navList">
<li><a href="../../../index.html?hadoop/hive/table/package-summary.html" target="_top">Frames</a></li>
<li><a href="package-summary.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
<p class="legalCopy"><small><font size=1>Copyright (c) 2011, 2015 Oracle and/or its affiliates.  All rights reserved.</font> </small></p>
</body>
</html>
